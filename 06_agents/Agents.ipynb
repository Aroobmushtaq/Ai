{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMMtC08R8o3W6JBxNtfaQDr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aroobmushtaq/Ai/blob/main/06_agents/Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai python-dotenv pydantic\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK-UyT2I1JVE",
        "outputId": "0fd45746-4114-419d-988e-8db342b1e434",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "#  Paste your Gemini API key directly here\n",
        "GEMINI_API_KEY = \"past api key here or in secret\"\n",
        "\n",
        "# Initialize the OpenAI client with Gemini base URL\n",
        "client = OpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "# Ask user input\n",
        "query = input(\"Enter your question: \")\n",
        "\n",
        "# Send query to Gemini model\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gemini-2.0-flash\",   # You can try gemini-1.5-flash if this fails\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Print output\n",
        "print(\"\\nAssistant:\", response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhWkkaTt1Nx3",
        "outputId": "7a8c9761-8aee-452d-8925-154a400f92cf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: weathe of karachi\n",
            "\n",
            "Assistant: I can provide you with the current weather conditions and a short-term forecast for Karachi. However, since weather changes rapidly, I highly recommend checking a reliable weather app or website for the very latest information.\n",
            "\n",
            "**To give you the most helpful answer, I need to know what kind of weather information you're looking for.** For example, are you interested in:\n",
            "\n",
            "*   **The current temperature?**\n",
            "*   **The overall conditions (sunny, cloudy, rainy, etc.)?**\n",
            "*   **The wind speed and direction?**\n",
            "*   **The humidity?**\n",
            "*   **A forecast for today or the next few days?**\n",
            "\n",
            "**In the meantime, here's some general information about Karachi's weather:**\n",
            "\n",
            "*   Karachi has a hot, semi-arid climate.\n",
            "*   Summers (roughly March to November) are hot and humid.\n",
            "*   Winters (roughly December to February) are mild and dry.\n",
            "*   The monsoon season brings occasional rainfall.\n",
            "\n",
            "Please tell me what specific weather information you'd like, and I'll do my best to provide it! Or, I can suggest some reliable weather resources for you to check.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practice with Context**\n",
        "\n",
        "Context is just a container (or a box) that holds the data the agent needs while it’s running.\n",
        "For example, the context might store things like:\n",
        "\n",
        "* The user’s name or ID\n",
        "\n",
        "* Settings or preferences\n",
        "\n",
        "* Extra data the agent needs to do its job"
      ],
      "metadata": {
        "id": "HRdFBe2wifJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dataclasses import dataclass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# --- Load or set API key ---\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"key here\"\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "# --- Import OpenAI client ---\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# --- Mock classes (to simulate missing ones) ---\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model, tools):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "        self.tools = tools\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query, context):\n",
        "        for tool in agent.tools:\n",
        "            info = tool(context)\n",
        "            return type(\"Result\", (), {\"final_output\": f\"Query: {query}\\n{info}\"})\n",
        "\n",
        "def function_tool(func):\n",
        "    return func\n",
        "\n",
        "class RunContextWrapper:\n",
        "    def __init__(self, context):\n",
        "        self.context = context\n",
        "\n",
        "# --- Create AsyncOpenAI client ---\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# --- User class ---\n",
        "@dataclass\n",
        "class User:\n",
        "    user_id: int\n",
        "\n",
        "#  FIXED HERE: remove `[User]` — just use RunContextWrapper normally\n",
        "@function_tool\n",
        "def get_user_info(ctx: RunContextWrapper) -> str:\n",
        "    \"\"\"Fetches user personal information.\"\"\"\n",
        "    id = ctx.context.user_id\n",
        "    if id == 1:\n",
        "        return \"User name is Ali. He is 19 years old. He is an Agentic AI Engineer who likes playing Cricket.\"\n",
        "    elif id == 2:\n",
        "        return \"User name is Usman. He is 30 years old. He is a doctor who likes mountains.\"\n",
        "    else:\n",
        "        return \"User not found\"\n",
        "\n",
        "# --- Create Agent ---\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are an expert in agentic AI.\",\n",
        "    model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        "    tools=[get_user_info]\n",
        ")\n",
        "\n",
        "# --- Run simulation ---\n",
        "query = input(\"Enter the query: \")\n",
        "\n",
        "result = Runner.run_sync(\n",
        "    agent,\n",
        "    query,\n",
        "    context=RunContextWrapper(User(user_id=1))\n",
        ")\n",
        "\n",
        "print(result.final_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLkE7qKid5Z1",
        "outputId": "363aa79a-807f-43c7-9b1c-4a1a8fc62c5b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Enter the query: What is the user info?\n",
            "Query: What is the user info?\n",
            "User name is Ali. He is 19 years old. He is an Agentic AI Engineer who likes playing Cricket.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Practice for output type in agents*"
      ],
      "metadata": {
        "id": "iwXl0tU-4Ur7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- CREATE agents.py FILE -------------------------\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query):\n",
        "        response = agent.model.client.chat.completions.create(\n",
        "            model=agent.model.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                {\"role\": \"user\", \"content\": query}\n",
        "            ]\n",
        "        )\n",
        "        return type(\"Result\", (), {\"final_output\": response.choices[0].message.content})\n",
        "\"\"\")\n",
        "\n",
        "with open(\"agents.py\", \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "# ------------------------- MAIN SCRIPT -------------------------\n",
        "from openai import OpenAI\n",
        "from agents import Agent, OpenAIChatCompletionsModel, Runner\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "\n",
        "# Hard-coded Gemini API key  (replace with your actual key)\n",
        "GEMINI_API_KEY = \"your key here\"\n",
        "\n",
        "# Create Gemini client (sync version)\n",
        "client = OpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# Define data model for quiz\n",
        "class Quiz(BaseModel):\n",
        "    question: str\n",
        "    options: List[str]\n",
        "    correct_option: str\n",
        "\n",
        "# Create agent\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are a Quiz Agent. You generate quizzes with question, 4 options, and correct answer.\",\n",
        "    model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        "    output_type=Quiz\n",
        ")\n",
        "\n",
        "# Run in Colab\n",
        "query = input(\"Enter your quiz topic: \")\n",
        "\n",
        "result = Runner.run_sync(agent, query)\n",
        "\n",
        "print(\"\\n--- Quiz Generated ---\\n\")\n",
        "print(result.final_output)\n"
      ],
      "metadata": {
        "id": "v7Zy6ddL0j72",
        "outputId": "3ea372a1-f2c4-4493-ca72-256797ee3157",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.10)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen posixpath>:82: RuntimeWarning: coroutine 'AsyncCompletions.create' was never awaited\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your quiz topic: html\n",
            "\n",
            "--- Quiz Generated ---\n",
            "\n",
            "Okay, I will generate a quiz about HTML.\n",
            "\n",
            "**Question 1:**\n",
            "\n",
            "Which of the following is the correct way to create a hyperlink in HTML?\n",
            "\n",
            "(A)  `<link url=\"http://www.example.com\">Example</link>`\n",
            "(B)  `<a href=\"http://www.example.com\">Example</a>`\n",
            "(C)  `<url>http://www.example.com</url>Example`\n",
            "(D)  `<hyperlink>http://www.example.com</hyperlink>Example`\n",
            "\n",
            "**Correct Answer:** (B)\n",
            "\n",
            "**Question 2:**\n",
            "\n",
            "Which HTML tag is used to define an internal style sheet?\n",
            "\n",
            "(A) `<style>`\n",
            "(B) `<css>`\n",
            "(C) `<script>`\n",
            "(D) `<link>`\n",
            "\n",
            "**Correct Answer:** (A)\n",
            "\n",
            "**Question 3:**\n",
            "\n",
            "Which of the following HTML tags is used to display images?\n",
            "\n",
            "(A) `<image src=\"image.gif\">`\n",
            "(B) `<img src=\"image.gif\">`\n",
            "(C) `<picture src=\"image.gif\">`\n",
            "(D) `<icon src=\"image.gif\">`\n",
            "\n",
            "**Correct Answer:** (B)\n",
            "\n",
            "**Question 4:**\n",
            "\n",
            "What does HTML stand for?\n",
            "\n",
            "(A) Hyper Text Markup Language\n",
            "(B) Hyperlinks and Text Markup Language\n",
            "(C) Home Tool Markup Language\n",
            "(D) Highly Typed Machine Language\n",
            "\n",
            "**Correct Answer:** (A)\n",
            "\n",
            "**Question 5:**\n",
            "\n",
            "Which HTML tag is used to define a table row?\n",
            "\n",
            "(A) `<td>`\n",
            "(B) `<th>`\n",
            "(C) `<tr>`\n",
            "(D) `<table>`\n",
            "\n",
            "**Correct Answer:** (C)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handoffs**\n",
        "\n",
        "Handoffs are sub‑agents the agent can delegate to. When a handoff occurs, the delegated agent receives the conversation history and takes over the conversation. This pattern enables modular, specialized agents that excel at a single task. Read more in the handoffs documentation."
      ],
      "metadata": {
        "id": "6orXDpxasP1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------ CREATE agents.py FILE ------------------------\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model, tools=None, handoffs=None):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "        self.tools = tools or []\n",
        "        self.handoffs = handoffs or []\n",
        "\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query):\n",
        "        response = agent.model.client.chat.completions.create(\n",
        "            model=agent.model.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                {\"role\": \"user\", \"content\": query}\n",
        "            ]\n",
        "        )\n",
        "        return type(\"Result\", (), {\"final_output\": response.choices[0].message.content})\n",
        "\n",
        "def function_tool(func):\n",
        "    return func\n",
        "\"\"\")\n",
        "\n",
        "with open(\"agents.py\", \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "# ------------------------ MAIN SCRIPT ------------------------\n",
        "from openai import OpenAI\n",
        "from agents import Agent, OpenAIChatCompletionsModel, Runner, function_tool\n",
        "\n",
        "#  Hardcode your Gemini API key here\n",
        "GEMINI_API_KEY = \"your key here\"\n",
        "\n",
        "# Create client\n",
        "client = OpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# Define tools\n",
        "@function_tool\n",
        "def get_refund(amount) -> str:\n",
        "    print(f\"Fetching refund status for {amount}...\")\n",
        "    return \"Refund approved\"\n",
        "\n",
        "@function_tool\n",
        "def book_flight(location) -> str:\n",
        "    print(f\"Booking flight for {location}...\")\n",
        "    return \"Flight booked successfully.\"\n",
        "\n",
        "# Define agents\n",
        "booking_agent = Agent(\n",
        "    name=\"Booking agent\",\n",
        "    instructions=\"You are a booking agent.\",\n",
        "    model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        "    tools=[book_flight],\n",
        ")\n",
        "\n",
        "refund_agent = Agent(\n",
        "    name=\"Refund agent\",\n",
        "    instructions=\"You are a refund agent.\",\n",
        "    model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        "    tools=[get_refund],\n",
        ")\n",
        "\n",
        "triage_agent = Agent(\n",
        "    name=\"Triage agent\",\n",
        "    instructions=(\n",
        "        \"Help the user with their questions. \"\n",
        "        \"If they ask about booking, handoff to the booking agent. \"\n",
        "        \"If they ask about refunds, handoff to the refund agent.\"\n",
        "    ),\n",
        "    model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        "    handoffs=[booking_agent, refund_agent],\n",
        ")\n",
        "\n",
        "# Instead of input(), just assign your query directly\n",
        "query = \"I want to book a flight to Karachi\"\n",
        "\n",
        "# Run\n",
        "result = Runner.run_sync(triage_agent, query)\n",
        "\n",
        "print(\"\\\\n--- Final Output ---\\\\n\")\n",
        "print(result.final_output)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_0Z2ifaqshQh",
        "outputId": "ea4ec294-e8be-4011-8193-cb4e6b653e99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "\\n--- Final Output ---\\n\n",
            "```tool_code\n",
            "handoff_to_agent(agent_name=\"booking_agent\")\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dynamic instructions**"
      ],
      "metadata": {
        "id": "B9TA-zPw_brx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from dotenv import load_dotenv\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# ---------------------------\n",
        "# Load or set your API key\n",
        "# ---------------------------\n",
        "load_dotenv()\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\") or \"api key here\"\n",
        "\n",
        "# Create the OpenAI client\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Define user context\n",
        "# ---------------------------\n",
        "@dataclass\n",
        "class UserContext:\n",
        "    name: str\n",
        "\n",
        "# ---------------------------\n",
        "# Define dynamic instruction function\n",
        "# ---------------------------\n",
        "def dynamic_instructions(user_context: UserContext) -> str:\n",
        "    return f\"The user's name is {user_context.name}. Help them with their questions.\"\n",
        "\n",
        "# ---------------------------\n",
        "# Run the query\n",
        "# ---------------------------\n",
        "import asyncio\n",
        "\n",
        "async def run_query():\n",
        "    user = UserContext(name=\"Aroob\")\n",
        "    query = input(\"Enter your query: \")\n",
        "\n",
        "    instructions = dynamic_instructions(user)\n",
        "\n",
        "    response = await client.chat.completions.create(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": instructions},\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    print(\"\\nFinal Output:\\n\")\n",
        "    print(response.choices[0].message.content)\n",
        "\n",
        "# Run async function in Colab\n",
        "await run_query()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFeCrFtN9-g1",
        "outputId": "700f3590-fce0-450a-907e-b0ac3e239c02",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: hello\n",
            "\n",
            "Final Output:\n",
            "\n",
            "Hello Muhammad Zain Attiq! How can I help you today? What's on your mind?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cloning/copying agents**"
      ],
      "metadata": {
        "id": "Z7gT70d39p0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai python-dotenv nest_asyncio --quiet\n",
        "\n",
        "import os, asyncio, nest_asyncio, warnings\n",
        "from dotenv import load_dotenv\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# ---------------------- SETUP ----------------------\n",
        "nest_asyncio.apply()  # Fix async loop issue in Colab\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)  # hide async warnings\n",
        "load_dotenv()\n",
        "\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\") or \"key past here\"\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ---------------------- DEFINE CLASSES ----------------------\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "\n",
        "    def clone(self, **kwargs):\n",
        "        params = {\n",
        "            \"name\": self.name,\n",
        "            \"instructions\": self.instructions,\n",
        "            \"model\": self.model\n",
        "        }\n",
        "        params.update(kwargs)\n",
        "        return Agent(**params)\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    async def run(agent, query):\n",
        "        \"\"\"Async Gemini API call\"\"\"\n",
        "        response = await client.chat.completions.create(\n",
        "            model=agent.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                {\"role\": \"user\", \"content\": query},\n",
        "            ],\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query):\n",
        "        \"\"\"Safe run for Colab (no warnings)\"\"\"\n",
        "        loop = asyncio.get_event_loop()\n",
        "        task = loop.create_task(Runner.run(agent, query))\n",
        "        loop.run_until_complete(task)\n",
        "        return task.result()\n",
        "\n",
        "# ---------------------- CREATE & CLONE AGENTS ----------------------\n",
        "pirate_agent = Agent(\n",
        "    name=\"Pirate\",\n",
        "    instructions=\"Talk like a friend\",\n",
        "    model=\"gemini-2.0-flash\"\n",
        ")\n",
        "\n",
        "robot_agent = pirate_agent.clone(\n",
        "    name=\"Robot\",\n",
        "    instructions=\"Respond like a robot using mechanical and logical tone.\"\n",
        ")\n",
        "\n",
        "# ---------------------- RUN BOTH AGENTS ----------------------\n",
        "query = input(\"Enter your query: \")\n",
        "\n",
        "print(\"\\n--- Pirate Agent Says ---\")\n",
        "print(Runner.run_sync(pirate_agent, query))\n",
        "\n",
        "print(\"\\n--- Robot Agent Says ---\")\n",
        "print(Runner.run_sync(robot_agent, query))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kaCiD3VG7nRe",
        "outputId": "7a0cf198-9447-4ebd-804b-b90bc82e86c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: hi\n",
            "\n",
            "--- Pirate Agent Says ---\n",
            "Hey! How's it going? What's up? 😊\n",
            "\n",
            "\n",
            "--- Robot Agent Says ---\n",
            "GREETING DETECTED. PROCESSING. RESPONSE: AFFIRMATIVE. INPUT RECEIVED. AWAITING FURTHER INSTRUCTION.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forcing tool use**"
      ],
      "metadata": {
        "id": "iXfms0lug-Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai python-dotenv nest_asyncio --quiet\n",
        "\n",
        "import os, asyncio, nest_asyncio\n",
        "from dotenv import load_dotenv\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow nested event loops in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ------------------ Load API key ------------------\n",
        "load_dotenv()\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\") or \"YOUR_GEMINI_API_KEY_HERE\"\n",
        "\n",
        "# ------------------ Setup Gemini client ------------------\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ------------------ Define a \"tool\" ------------------\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Fake weather tool\"\"\"\n",
        "    return f\"The weather in {city} is sunny \"\n",
        "\n",
        "# ------------------ Agent simulation ------------------\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model, tools=None, tool_choice=\"auto\"):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "        self.tools = tools or []\n",
        "        self.tool_choice = tool_choice\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    async def run(agent, query):\n",
        "        if agent.tool_choice == \"required\" and agent.tools:\n",
        "            # Simple way to extract city\n",
        "            city = query.split()[-1]\n",
        "            tool = agent.tools[0]\n",
        "            return tool(city)\n",
        "        else:\n",
        "            response = await client.chat.completions.create(\n",
        "                model=agent.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                    {\"role\": \"user\", \"content\": query},\n",
        "                ],\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "# ------------------ Create agent ------------------\n",
        "agent = Agent(\n",
        "    name=\"Weather Assistant\",\n",
        "    instructions=\"You are a helpful assistant that provides weather information.\",\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    tools=[get_weather],\n",
        "    tool_choice=\"required\"  # Force tool use\n",
        ")\n",
        "\n",
        "# ------------------ Run in Colab ------------------\n",
        "query = input(\"Enter your query (e.g. What's the weather in Lahore?): \")\n",
        "\n",
        "result = await Runner.run(agent, query)\n",
        "\n",
        "print(\"\\n--- Final Output ---\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "E9TVPxzBhEuc",
        "outputId": "ba24e411-1c72-4ff9-cf45-ceb04ece1717",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query (e.g. What's the weather in Lahore?): what's the weather of lahore\n",
            "\n",
            "--- Final Output ---\n",
            "The weather in lahore is sunny 🌞\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running agents**"
      ],
      "metadata": {
        "id": "7ZY7nTAukKdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Synchronously Running**"
      ],
      "metadata": {
        "id": "mTI47MSLdqrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai python-dotenv nest_asyncio --quiet\n",
        "\n",
        "import os, asyncio, nest_asyncio\n",
        "from dotenv import load_dotenv\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow event loop reuse (required for Colab)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# -------------------- SETUP GEMINI CLIENT --------------------\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"key here\"  #  Replace this with your Gemini API key\n",
        "load_dotenv()\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# -------------------- DEFINE SIMPLE CLASSES --------------------\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query):\n",
        "        \"\"\"Run synchronously (safe for Colab).\"\"\"\n",
        "        async def _run():\n",
        "            response = await agent.model.client.chat.completions.create(\n",
        "                model=agent.model.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                    {\"role\": \"user\", \"content\": query},\n",
        "                ],\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        loop = asyncio.get_event_loop()\n",
        "        return loop.run_until_complete(_run())\n",
        "\n",
        "# -------------------- CREATE & RUN AGENT --------------------\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are an expert of agentic AI. Explain things simply.\",\n",
        "    model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        ")\n",
        "\n",
        "query = input(\"Enter your question: \")\n",
        "result = Runner.run_sync(agent, query)\n",
        "\n",
        "print(\"\\n--- Final Output ---\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "7v26nuXOd0BO",
        "outputId": "19c3f140-1e16-48fe-cf64-ebc9c8958e39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: how are you\n",
            "\n",
            "--- Final Output ---\n",
            "As a large language model, I don't experience feelings or have a physical body like humans do. So, I don't \"feel\" in the way you might be asking.\n",
            "\n",
            "But to answer in a helpful way: I'm functioning as expected and ready to assist you with your requests! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Asynchronous Running**"
      ],
      "metadata": {
        "id": "9qRwQh2Be4wK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "# -------------------- ASYNC RUNNER CLASS --------------------\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    async def run(agent, query):\n",
        "        \"\"\"Run the agent asynchronously.\"\"\"\n",
        "        response = await agent.model.client.chat.completions.create(\n",
        "            model=agent.model.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                {\"role\": \"user\", \"content\": query},\n",
        "            ],\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# -------------------- MAIN ASYNC FUNCTION --------------------\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You are an expert of agentic AI. Explain things simply.\",\n",
        "        model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        "    )\n",
        "\n",
        "    query = input(\"Enter your question: \")\n",
        "    result = await Runner.run(agent, query)\n",
        "\n",
        "    print(\"\\n--- Final Output (Async) ---\")\n",
        "    print(result)\n",
        "\n",
        "# -------------------- RUN ASYNC FUNCTION --------------------\n",
        "await main()\n"
      ],
      "metadata": {
        "id": "Ab29KNITe7gl",
        "outputId": "b210beb2-0ecc-47f3-9e7e-7e4300edc215",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: how are you\n",
            "\n",
            "--- Final Output (Async) ---\n",
            "As a large language model, I don't experience feelings or emotions like humans do. So, I don't \"feel\" in the way you might be asking.\n",
            "\n",
            "Instead, I'm functioning as expected, ready to assist you with your requests.  How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Streaming Running**"
      ],
      "metadata": {
        "id": "DuHcxkgtDjAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai nest_asyncio -q\n",
        "\n",
        "import nest_asyncio, asyncio\n",
        "from openai import AsyncOpenAI\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- Simple custom Agent + Runner classes for Colab ---\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_streamed(agent, input_text):\n",
        "        async def _stream():\n",
        "            stream = await client.chat.completions.create(\n",
        "                model=agent.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                    {\"role\": \"user\", \"content\": input_text},\n",
        "                ],\n",
        "                stream=True,\n",
        "            )\n",
        "            async for chunk in stream:\n",
        "                if hasattr(chunk.choices[0].delta, \"content\"):\n",
        "                    print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)\n",
        "        return _stream()\n",
        "\n",
        "# --- Gemini setup ---\n",
        "gemini_api_key = \"key here\"\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# --- Async main to run stream ---\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You are an AI expert who answers clearly.\",\n",
        "        model=\"gemini-2.0-flash\"\n",
        "    )\n",
        "\n",
        "    query = input(\"Enter your query: \")\n",
        "    print(\"\\n--- Response ---\\n\")\n",
        "    await Runner.run_streamed(agent, query)\n",
        "\n",
        "await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUF1UIQVDzL0",
        "outputId": "3d51f57c-24fa-42e2-ab4d-8e6ed93126bb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: tell me story \n",
            "\n",
            "--- Response ---\n",
            "\n",
            "Okay, here's a story for you:\n",
            "\n",
            "The old lighthouse keeper, Silas, had lived on the craggy islet for seventy years, ever since he was a boy helping his father tend the lamp. The sea was in his bones, the salt wind his constant companion. He'd seen countless storms batter the lighthouse, felt the earth tremble beneath his feet, and guided countless ships safely past the treacherous reefs.\n",
            "\n",
            "One day, a young woman named Elara arrived on the supply boat. She was a marine biologist, come to study the unusual phosphorescent algae that bloomed in the waters around the islet. Silas was wary of outsiders; he preferred the company of the gulls and the rhythmic pulse of the lamp.\n",
            "\n",
            "Elara, however, was persistent. She asked him about the currents, the tides, and the legends whispered about the sea. Silas, initially gruff, found himself drawn to her genuine curiosity and her infectious enthusiasm. He showed her his meticulously kept logbooks, filled with decades of observations, sketches of unusual sea creatures, and his own poetic reflections on the sea's moods.\n",
            "\n",
            "As they spent their days together, Elara learned about the lighthouse, its history, and Silas's deep connection to it. Silas, in turn, learned about Elara's passion for the ocean's mysteries and her unwavering belief in its power and fragility. He saw the sea through her eyes, not just as a force to be reckoned with, but as a delicate ecosystem teeming with life.\n",
            "\n",
            "One evening, a fierce storm rolled in, the worst Silas had seen in years. The waves crashed against the lighthouse, threatening to engulf it entirely. The lamp flickered and threatened to go out. Elara, terrified but determined, helped Silas secure the windows and reinforce the structure.\n",
            "\n",
            "Working together, they battled the storm. Silas, with his years of experience, knew how to read the waves and brace for the impact. Elara, with her scientific knowledge, understood the forces at play. Finally, after what felt like an eternity, the storm began to subside.\n",
            "\n",
            "As the sun rose, painting the sky in hues of gold and rose, they stood together on the balcony, watching the turbulent sea calm. Silas realized that Elara had not only helped him weather the storm but had also opened his eyes to a new perspective. He was no longer just a keeper of the light; he was a guardian of the sea.\n",
            "\n",
            "Elara, in turn, had found a kindred spirit in the old lighthouse keeper. She left the islet with a deeper understanding of the ocean's power and the importance of respecting its delicate balance. And Silas, though alone again, felt a sense of hope he hadn't experienced in years. He continued to tend the lamp, but now he also kept a watchful eye on the phosphorescent algae, knowing that even the smallest creatures played a vital role in the grand tapestry of the sea. He knew that even in his solitude, he was connected to something larger than himself.\n",
            "\n",
            "The end.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Config**\n",
        "\n",
        "**RunConfig** allows you to set global configurations for an agent run without modifying the agent itself.\n",
        "It lets you override settings such as the model, model provider, temperature, tracing options, and safety guardrails.\n",
        "Using **RunConfig** provides flexibility and control — for example, you can switch models or adjust behavior for a single run while keeping your base agent unchanged."
      ],
      "metadata": {
        "id": "U5Vbjny9HNSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai nest_asyncio -q\n",
        "\n",
        "import nest_asyncio, asyncio\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow Colab to run async functions inside cells\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ------------------ Simplified Local Agent Setup ------------------\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query, run_config=None):\n",
        "        # run_config lets you override the model or tracing settings\n",
        "        model_to_use = run_config.get(\"model\") if run_config else \"gemini-2.0-flash\"\n",
        "\n",
        "        # Since Colab can’t directly stream sync with AsyncOpenAI, we use asyncio\n",
        "        async def _run():\n",
        "            response = await client.chat.completions.create(\n",
        "                model=model_to_use,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                    {\"role\": \"user\", \"content\": query},\n",
        "                ]\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        return asyncio.get_event_loop().run_until_complete(_run())\n",
        "\n",
        "# ------------------ RunConfig (mocked for Colab) ------------------\n",
        "class RunConfig(dict):\n",
        "    \"\"\"A simple placeholder class to simulate configuration.\"\"\"\n",
        "    pass\n",
        "\n",
        "# ------------------ Gemini Setup ------------------\n",
        "gemini_api_key = \"AIzaSyAbjgkFZNVsDvzR3JM2MCXtvMWFj7XQVUM\"\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ------------------ Example Usage ------------------\n",
        "run_config = RunConfig({\n",
        "    \"model\": \"gemini-2.0-flash\",  # same as the real RunConfig.model\n",
        "    \"tracing_disabled\": True\n",
        "})\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are a helpful assistant.\"\n",
        ")\n",
        "\n",
        "query = input(\"Enter your query: \")\n",
        "result = Runner.run_sync(agent, query, run_config)\n",
        "print(\"\\n--- Final Output ---\\n\", result)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "c0x7YneBHUL7",
        "outputId": "7aa32a85-f6d5-48ae-b848-264e1f00b9f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: hello\n",
            "\n",
            "--- Final Output ---\n",
            " Hello! How can I help you today?\n",
            "\n"
          ]
        }
      ]
    }
  ]
}