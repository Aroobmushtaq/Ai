{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPb+pDuszmmMzX0FWnOhW3p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aroobmushtaq/Ai/blob/main/06_agents/Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai python-dotenv pydantic\n"
      ],
      "metadata": {
        "id": "CK-UyT2I1JVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "#  Paste your Gemini API key directly here\n",
        "GEMINI_API_KEY = \"past api key here or in secret\"\n",
        "\n",
        "# Initialize the OpenAI client with Gemini base URL\n",
        "client = OpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "# Ask user input\n",
        "query = input(\"Enter your question: \")\n",
        "\n",
        "# Send query to Gemini model\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gemini-2.0-flash\",   # You can try gemini-1.5-flash if this fails\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Print output\n",
        "print(\"\\nAssistant:\", response.choices[0].message.content)\n"
      ],
      "metadata": {
        "id": "lhWkkaTt1Nx3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practice with Context**\n",
        "\n",
        "Context is just a container (or a box) that holds the data the agent needs while itâ€™s running.\n",
        "For example, the context might store things like:\n",
        "\n",
        "* The userâ€™s name or ID\n",
        "\n",
        "* Settings or preferences\n",
        "\n",
        "* Extra data the agent needs to do its job"
      ],
      "metadata": {
        "id": "HRdFBe2wifJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dataclasses import dataclass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# --- Load or set API key ---\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"key here\"\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "# --- Import OpenAI client ---\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# --- Mock classes (to simulate missing ones) ---\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model, tools):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "        self.tools = tools\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query, context):\n",
        "        for tool in agent.tools:\n",
        "            info = tool(context)\n",
        "            return type(\"Result\", (), {\"final_output\": f\"Query: {query}\\n{info}\"})\n",
        "\n",
        "def function_tool(func):\n",
        "    return func\n",
        "\n",
        "class RunContextWrapper:\n",
        "    def __init__(self, context):\n",
        "        self.context = context\n",
        "\n",
        "# --- Create AsyncOpenAI client ---\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# --- User class ---\n",
        "@dataclass\n",
        "class User:\n",
        "    user_id: int\n",
        "\n",
        "#  FIXED HERE: remove `[User]` â€” just use RunContextWrapper normally\n",
        "@function_tool\n",
        "def get_user_info(ctx: RunContextWrapper) -> str:\n",
        "    \"\"\"Fetches user personal information.\"\"\"\n",
        "    id = ctx.context.user_id\n",
        "    if id == 1:\n",
        "        return \"User name is Ali. He is 19 years old. He is an Agentic AI Engineer who likes playing Cricket.\"\n",
        "    elif id == 2:\n",
        "        return \"User name is Usman. He is 30 years old. He is a doctor who likes mountains.\"\n",
        "    else:\n",
        "        return \"User not found\"\n",
        "\n",
        "# --- Create Agent ---\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are an expert in agentic AI.\",\n",
        "    model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        "    tools=[get_user_info]\n",
        ")\n",
        "\n",
        "# --- Run simulation ---\n",
        "query = input(\"Enter the query: \")\n",
        "\n",
        "result = Runner.run_sync(\n",
        "    agent,\n",
        "    query,\n",
        "    context=RunContextWrapper(User(user_id=1))\n",
        ")\n",
        "\n",
        "print(result.final_output)\n"
      ],
      "metadata": {
        "id": "mLkE7qKid5Z1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Practice for output type in agents*"
      ],
      "metadata": {
        "id": "iwXl0tU-4Ur7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- CREATE agents.py FILE -------------------------\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query):\n",
        "        response = agent.model.client.chat.completions.create(\n",
        "            model=agent.model.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                {\"role\": \"user\", \"content\": query}\n",
        "            ]\n",
        "        )\n",
        "        return type(\"Result\", (), {\"final_output\": response.choices[0].message.content})\n",
        "\"\"\")\n",
        "\n",
        "with open(\"agents.py\", \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "# ------------------------- MAIN SCRIPT -------------------------\n",
        "from openai import OpenAI\n",
        "from agents import Agent, OpenAIChatCompletionsModel, Runner\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "\n",
        "# Hard-coded Gemini API key  (replace with your actual key)\n",
        "GEMINI_API_KEY = \"your key here\"\n",
        "\n",
        "# Create Gemini client (sync version)\n",
        "client = OpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# Define data model for quiz\n",
        "class Quiz(BaseModel):\n",
        "    question: str\n",
        "    options: List[str]\n",
        "    correct_option: str\n",
        "\n",
        "# Create agent\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are a Quiz Agent. You generate quizzes with question, 4 options, and correct answer.\",\n",
        "    model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        "    output_type=Quiz\n",
        ")\n",
        "\n",
        "# Run in Colab\n",
        "query = input(\"Enter your quiz topic: \")\n",
        "\n",
        "result = Runner.run_sync(agent, query)\n",
        "\n",
        "print(\"\\n--- Quiz Generated ---\\n\")\n",
        "print(result.final_output)\n"
      ],
      "metadata": {
        "id": "v7Zy6ddL0j72",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handoffs**\n",
        "\n",
        "Handoffs are subâ€‘agents the agent can delegate to. When a handoff occurs, the delegated agent receives the conversation history and takes over the conversation. This pattern enables modular, specialized agents that excel at a single task. Read more in the handoffs documentation."
      ],
      "metadata": {
        "id": "6orXDpxasP1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------ CREATE agents.py FILE ------------------------\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model, tools=None, handoffs=None):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "        self.tools = tools or []\n",
        "        self.handoffs = handoffs or []\n",
        "\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query):\n",
        "        response = agent.model.client.chat.completions.create(\n",
        "            model=agent.model.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                {\"role\": \"user\", \"content\": query}\n",
        "            ]\n",
        "        )\n",
        "        return type(\"Result\", (), {\"final_output\": response.choices[0].message.content})\n",
        "\n",
        "def function_tool(func):\n",
        "    return func\n",
        "\"\"\")\n",
        "\n",
        "with open(\"agents.py\", \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "# ------------------------ MAIN SCRIPT ------------------------\n",
        "from openai import OpenAI\n",
        "from agents import Agent, OpenAIChatCompletionsModel, Runner, function_tool\n",
        "\n",
        "#  Hardcode your Gemini API key here\n",
        "GEMINI_API_KEY = \"your key here\"\n",
        "\n",
        "# Create client\n",
        "client = OpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# Define tools\n",
        "@function_tool\n",
        "def get_refund(amount) -> str:\n",
        "    print(f\"Fetching refund status for {amount}...\")\n",
        "    return \"Refund approved\"\n",
        "\n",
        "@function_tool\n",
        "def book_flight(location) -> str:\n",
        "    print(f\"Booking flight for {location}...\")\n",
        "    return \"Flight booked successfully.\"\n",
        "\n",
        "# Define agents\n",
        "booking_agent = Agent(\n",
        "    name=\"Booking agent\",\n",
        "    instructions=\"You are a booking agent.\",\n",
        "    model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        "    tools=[book_flight],\n",
        ")\n",
        "\n",
        "refund_agent = Agent(\n",
        "    name=\"Refund agent\",\n",
        "    instructions=\"You are a refund agent.\",\n",
        "    model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        "    tools=[get_refund],\n",
        ")\n",
        "\n",
        "triage_agent = Agent(\n",
        "    name=\"Triage agent\",\n",
        "    instructions=(\n",
        "        \"Help the user with their questions. \"\n",
        "        \"If they ask about booking, handoff to the booking agent. \"\n",
        "        \"If they ask about refunds, handoff to the refund agent.\"\n",
        "    ),\n",
        "    model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        "    handoffs=[booking_agent, refund_agent],\n",
        ")\n",
        "\n",
        "# Instead of input(), just assign your query directly\n",
        "query = \"I want to book a flight to Karachi\"\n",
        "\n",
        "# Run\n",
        "result = Runner.run_sync(triage_agent, query)\n",
        "\n",
        "print(\"\\\\n--- Final Output ---\\\\n\")\n",
        "print(result.final_output)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_0Z2ifaqshQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dynamic instructions**"
      ],
      "metadata": {
        "id": "B9TA-zPw_brx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from dotenv import load_dotenv\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# ---------------------------\n",
        "# Load or set your API key\n",
        "# ---------------------------\n",
        "load_dotenv()\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\") or \"api key here\"\n",
        "\n",
        "# Create the OpenAI client\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Define user context\n",
        "# ---------------------------\n",
        "@dataclass\n",
        "class UserContext:\n",
        "    name: str\n",
        "\n",
        "# ---------------------------\n",
        "# Define dynamic instruction function\n",
        "# ---------------------------\n",
        "def dynamic_instructions(user_context: UserContext) -> str:\n",
        "    return f\"The user's name is {user_context.name}. Help them with their questions.\"\n",
        "\n",
        "# ---------------------------\n",
        "# Run the query\n",
        "# ---------------------------\n",
        "import asyncio\n",
        "\n",
        "async def run_query():\n",
        "    user = UserContext(name=\"Aroob\")\n",
        "    query = input(\"Enter your query: \")\n",
        "\n",
        "    instructions = dynamic_instructions(user)\n",
        "\n",
        "    response = await client.chat.completions.create(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": instructions},\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    print(\"\\nFinal Output:\\n\")\n",
        "    print(response.choices[0].message.content)\n",
        "\n",
        "# Run async function in Colab\n",
        "await run_query()\n"
      ],
      "metadata": {
        "id": "RFeCrFtN9-g1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cloning/copying agents**"
      ],
      "metadata": {
        "id": "Z7gT70d39p0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai python-dotenv nest_asyncio --quiet\n",
        "\n",
        "import os, asyncio, nest_asyncio, warnings\n",
        "from dotenv import load_dotenv\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# ---------------------- SETUP ----------------------\n",
        "nest_asyncio.apply()  # Fix async loop issue in Colab\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)  # hide async warnings\n",
        "load_dotenv()\n",
        "\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\") or \"key past here\"\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ---------------------- DEFINE CLASSES ----------------------\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "\n",
        "    def clone(self, **kwargs):\n",
        "        params = {\n",
        "            \"name\": self.name,\n",
        "            \"instructions\": self.instructions,\n",
        "            \"model\": self.model\n",
        "        }\n",
        "        params.update(kwargs)\n",
        "        return Agent(**params)\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    async def run(agent, query):\n",
        "        \"\"\"Async Gemini API call\"\"\"\n",
        "        response = await client.chat.completions.create(\n",
        "            model=agent.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                {\"role\": \"user\", \"content\": query},\n",
        "            ],\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query):\n",
        "        \"\"\"Safe run for Colab (no warnings)\"\"\"\n",
        "        loop = asyncio.get_event_loop()\n",
        "        task = loop.create_task(Runner.run(agent, query))\n",
        "        loop.run_until_complete(task)\n",
        "        return task.result()\n",
        "\n",
        "# ---------------------- CREATE & CLONE AGENTS ----------------------\n",
        "pirate_agent = Agent(\n",
        "    name=\"Pirate\",\n",
        "    instructions=\"Talk like a friend\",\n",
        "    model=\"gemini-2.0-flash\"\n",
        ")\n",
        "\n",
        "robot_agent = pirate_agent.clone(\n",
        "    name=\"Robot\",\n",
        "    instructions=\"Respond like a robot using mechanical and logical tone.\"\n",
        ")\n",
        "\n",
        "# ---------------------- RUN BOTH AGENTS ----------------------\n",
        "query = input(\"Enter your query: \")\n",
        "\n",
        "print(\"\\n--- Pirate Agent Says ---\")\n",
        "print(Runner.run_sync(pirate_agent, query))\n",
        "\n",
        "print(\"\\n--- Robot Agent Says ---\")\n",
        "print(Runner.run_sync(robot_agent, query))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kaCiD3VG7nRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forcing tool use**"
      ],
      "metadata": {
        "id": "iXfms0lug-Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai python-dotenv nest_asyncio --quiet\n",
        "\n",
        "import os, asyncio, nest_asyncio\n",
        "from dotenv import load_dotenv\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow nested event loops in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ------------------ Load API key ------------------\n",
        "load_dotenv()\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\") or \"YOUR_GEMINI_API_KEY_HERE\"\n",
        "\n",
        "# ------------------ Setup Gemini client ------------------\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ------------------ Define a \"tool\" ------------------\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Fake weather tool\"\"\"\n",
        "    return f\"The weather in {city} is sunny \"\n",
        "\n",
        "# ------------------ Agent simulation ------------------\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model, tools=None, tool_choice=\"auto\"):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "        self.tools = tools or []\n",
        "        self.tool_choice = tool_choice\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    async def run(agent, query):\n",
        "        if agent.tool_choice == \"required\" and agent.tools:\n",
        "            # Simple way to extract city\n",
        "            city = query.split()[-1]\n",
        "            tool = agent.tools[0]\n",
        "            return tool(city)\n",
        "        else:\n",
        "            response = await client.chat.completions.create(\n",
        "                model=agent.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                    {\"role\": \"user\", \"content\": query},\n",
        "                ],\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "# ------------------ Create agent ------------------\n",
        "agent = Agent(\n",
        "    name=\"Weather Assistant\",\n",
        "    instructions=\"You are a helpful assistant that provides weather information.\",\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    tools=[get_weather],\n",
        "    tool_choice=\"required\"  # Force tool use\n",
        ")\n",
        "\n",
        "# ------------------ Run in Colab ------------------\n",
        "query = input(\"Enter your query (e.g. What's the weather in Lahore?): \")\n",
        "\n",
        "result = await Runner.run(agent, query)\n",
        "\n",
        "print(\"\\n--- Final Output ---\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "E9TVPxzBhEuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running agents**"
      ],
      "metadata": {
        "id": "7ZY7nTAukKdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Synchronously Running**"
      ],
      "metadata": {
        "id": "mTI47MSLdqrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai python-dotenv nest_asyncio --quiet\n",
        "\n",
        "import os, asyncio, nest_asyncio\n",
        "from dotenv import load_dotenv\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow event loop reuse (required for Colab)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# -------------------- SETUP GEMINI CLIENT --------------------\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"key here\"  #  Replace this with your Gemini API key\n",
        "load_dotenv()\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# -------------------- DEFINE SIMPLE CLASSES --------------------\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query):\n",
        "        \"\"\"Run synchronously (safe for Colab).\"\"\"\n",
        "        async def _run():\n",
        "            response = await agent.model.client.chat.completions.create(\n",
        "                model=agent.model.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                    {\"role\": \"user\", \"content\": query},\n",
        "                ],\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        loop = asyncio.get_event_loop()\n",
        "        return loop.run_until_complete(_run())\n",
        "\n",
        "# -------------------- CREATE & RUN AGENT --------------------\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are an expert of agentic AI. Explain things simply.\",\n",
        "    model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        ")\n",
        "\n",
        "query = input(\"Enter your question: \")\n",
        "result = Runner.run_sync(agent, query)\n",
        "\n",
        "print(\"\\n--- Final Output ---\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "7v26nuXOd0BO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Asynchronous Running**"
      ],
      "metadata": {
        "id": "9qRwQh2Be4wK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "# -------------------- ASYNC RUNNER CLASS --------------------\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    async def run(agent, query):\n",
        "        \"\"\"Run the agent asynchronously.\"\"\"\n",
        "        response = await agent.model.client.chat.completions.create(\n",
        "            model=agent.model.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                {\"role\": \"user\", \"content\": query},\n",
        "            ],\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# -------------------- MAIN ASYNC FUNCTION --------------------\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You are an expert of agentic AI. Explain things simply.\",\n",
        "        model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        "    )\n",
        "\n",
        "    query = input(\"Enter your question: \")\n",
        "    result = await Runner.run(agent, query)\n",
        "\n",
        "    print(\"\\n--- Final Output (Async) ---\")\n",
        "    print(result)\n",
        "\n",
        "# -------------------- RUN ASYNC FUNCTION --------------------\n",
        "await main()\n"
      ],
      "metadata": {
        "id": "Ab29KNITe7gl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Streaming Running**"
      ],
      "metadata": {
        "id": "DuHcxkgtDjAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai nest_asyncio -q\n",
        "\n",
        "import nest_asyncio, asyncio\n",
        "from openai import AsyncOpenAI\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- Simple custom Agent + Runner classes for Colab ---\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_streamed(agent, input_text):\n",
        "        async def _stream():\n",
        "            stream = await client.chat.completions.create(\n",
        "                model=agent.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                    {\"role\": \"user\", \"content\": input_text},\n",
        "                ],\n",
        "                stream=True,\n",
        "            )\n",
        "            async for chunk in stream:\n",
        "                if hasattr(chunk.choices[0].delta, \"content\"):\n",
        "                    print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)\n",
        "        return _stream()\n",
        "\n",
        "# --- Gemini setup ---\n",
        "gemini_api_key = \"key here\"\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# --- Async main to run stream ---\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You are an AI expert who answers clearly.\",\n",
        "        model=\"gemini-2.0-flash\"\n",
        "    )\n",
        "\n",
        "    query = input(\"Enter your query: \")\n",
        "    print(\"\\n--- Response ---\\n\")\n",
        "    await Runner.run_streamed(agent, query)\n",
        "\n",
        "await main()\n"
      ],
      "metadata": {
        "id": "eUF1UIQVDzL0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Config**\n",
        "\n",
        "**RunConfig** allows you to set global configurations for an agent run without modifying the agent itself.\n",
        "It lets you override settings such as the model, model provider, temperature, tracing options, and safety guardrails.\n",
        "Using **RunConfig** provides flexibility and control â€” for example, you can switch models or adjust behavior for a single run while keeping your base agent unchanged."
      ],
      "metadata": {
        "id": "U5Vbjny9HNSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai nest_asyncio -q\n",
        "\n",
        "import nest_asyncio, asyncio\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow Colab to run async functions inside cells\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ------------------ Simplified Local Agent Setup ------------------\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query, run_config=None):\n",
        "        # run_config lets you override the model or tracing settings\n",
        "        model_to_use = run_config.get(\"model\") if run_config else \"gemini-2.0-flash\"\n",
        "\n",
        "        # Since Colab canâ€™t directly stream sync with AsyncOpenAI, we use asyncio\n",
        "        async def _run():\n",
        "            response = await client.chat.completions.create(\n",
        "                model=model_to_use,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                    {\"role\": \"user\", \"content\": query},\n",
        "                ]\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        return asyncio.get_event_loop().run_until_complete(_run())\n",
        "\n",
        "# ------------------ RunConfig (mocked for Colab) ------------------\n",
        "class RunConfig(dict):\n",
        "    \"\"\"A simple placeholder class to simulate configuration.\"\"\"\n",
        "    pass\n",
        "\n",
        "# ------------------ Gemini Setup ------------------\n",
        "gemini_api_key = \"key here\"\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ------------------ Example Usage ------------------\n",
        "run_config = RunConfig({\n",
        "    \"model\": \"gemini-2.0-flash\",  # same as the real RunConfig.model\n",
        "    \"tracing_disabled\": True\n",
        "})\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are a helpful assistant.\"\n",
        ")\n",
        "\n",
        "query = input(\"Enter your query: \")\n",
        "result = Runner.run_sync(agent, query, run_config)\n",
        "print(\"\\n--- Final Output ---\\n\", result)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "c0x7YneBHUL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chat Threads**"
      ],
      "metadata": {
        "id": "LMQpE01aZaS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai nest_asyncio -q\n",
        "\n",
        "import nest_asyncio, asyncio, os\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow async functions in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ------------------ Simple local replacements for agents ------------------\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, messages):\n",
        "        \"\"\"Run a conversation synchronously with memory.\"\"\"\n",
        "        async def _run():\n",
        "            response = await agent.model.client.chat.completions.create(\n",
        "                model=agent.model.model,\n",
        "                messages=messages\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        return asyncio.get_event_loop().run_until_complete(_run())\n",
        "\n",
        "# ------------------ Gemini Setup ------------------\n",
        "gemini_api_key = \"key here\"  # or load from env\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ------------------ Agent Setup ------------------\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are an expert of agentic AI.\",\n",
        "    model=OpenAIChatCompletionsModel(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        openai_client=client\n",
        "    )\n",
        ")\n",
        "\n",
        "# ------------------ Conversation with Memory ------------------\n",
        "history = [{\"role\": \"system\", \"content\": agent.instructions}]\n",
        "\n",
        "while True:\n",
        "    query = input(\"Enter the query (or 'exit' to stop): \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    history.append({\"role\": \"user\", \"content\": query})\n",
        "\n",
        "    result = Runner.run_sync(agent, history)\n",
        "\n",
        "    history.append({\"role\": \"assistant\", \"content\": result})\n",
        "\n",
        "    print(\"\\n--- Assistant ---\\n\", result, \"\\n\")\n"
      ],
      "metadata": {
        "id": "S_ETErw_Ziiq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Raw Response Event**"
      ],
      "metadata": {
        "id": "86Hq5CjMBMi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai nest_asyncio -q\n",
        "\n",
        "import nest_asyncio, asyncio, os, time, sys\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow async inside Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ------------------ Agent + Runner Classes ------------------\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, messages):\n",
        "        \"\"\"Runs a chat completion synchronously.\"\"\"\n",
        "        async def _run():\n",
        "            response = await agent.model.client.chat.completions.create(\n",
        "                model=agent.model.model,\n",
        "                messages=messages\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        return asyncio.get_event_loop().run_until_complete(_run())\n",
        "\n",
        "# ------------------ Gemini Setup ------------------\n",
        "gemini_api_key = \"key here\"  #  Replace with your API key\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ------------------ Agent Setup ------------------\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are an expert AI assistant who explains concepts clearly.\",\n",
        "    model=OpenAIChatCompletionsModel(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        openai_client=client\n",
        "    )\n",
        ")\n",
        "\n",
        "# ------------------ Chat Loop ------------------\n",
        "history = [{\"role\": \"system\", \"content\": agent.instructions}]\n",
        "\n",
        "while True:\n",
        "    query = input(\"Enter your question (or 'exit' to quit): \")\n",
        "    if query.lower() == \"exit\":\n",
        "        print(\"Goodbye! ðŸ‘‹\")\n",
        "        break\n",
        "\n",
        "    history.append({\"role\": \"user\", \"content\": query})\n",
        "    result = Runner.run_sync(agent, history)\n",
        "    history.append({\"role\": \"assistant\", \"content\": result})\n",
        "\n",
        "    # Fake typing effect for realism\n",
        "    print(\"\\n--- Assistant ---\\n\")\n",
        "    for ch in result:\n",
        "        print(ch, end=\"\")\n",
        "        sys.stdout.flush()\n",
        "        time.sleep(0.01)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "c-oCp9BwAFJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**run_item_and_agent_events**"
      ],
      "metadata": {
        "id": "LWC8XRTqUZh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai nest_asyncio python-dotenv -q\n",
        "\n",
        "import nest_asyncio, asyncio, os, sys, time, random\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow async in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ------------------ Custom lightweight framework ------------------\n",
        "\n",
        "class ItemHelpers:\n",
        "    @staticmethod\n",
        "    def text_message_output(item):\n",
        "        return item.get(\"content\", \"\")\n",
        "\n",
        "# ------------------ Custom Agent system ------------------\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model, tools=None):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "        self.tools = tools or []\n",
        "\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    async def run_streamed(agent, query):\n",
        "        \"\"\"Simulate streamed run events manually.\"\"\"\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": agent.instructions},\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "        ]\n",
        "\n",
        "        # Fake stream behavior (since openai-python doesnâ€™t expose granular events easily)\n",
        "        print(\"=== Run starting ===\")\n",
        "\n",
        "        # 1ï¸âƒ£ Detect if query involves a weather tool call\n",
        "        if \"weather\" in query.lower():\n",
        "            print(\"-- Tool was called: get_weather\")\n",
        "            output = agent.tools[0](\"Lahore\")  # simulate tool output\n",
        "            await asyncio.sleep(1)\n",
        "            print(f\"-- Tool output: {output}\")\n",
        "            messages.append({\"role\": \"assistant\", \"content\": output})\n",
        "        else:\n",
        "            print(\"-- No tool call needed\")\n",
        "\n",
        "        # 2ï¸âƒ£ Call model\n",
        "        response = await agent.model.client.chat.completions.create(\n",
        "            model=agent.model.model,\n",
        "            messages=messages\n",
        "        )\n",
        "        text = response.choices[0].message.content\n",
        "        print(\"-- Message output:\")\n",
        "        print(text)\n",
        "\n",
        "        print(\"=== Run complete ===\")\n",
        "        return text\n",
        "\n",
        "# ------------------ Gemini Setup ------------------\n",
        "gemini_api_key = \"YOUR_GEMINI_API_KEY\"  # ðŸ”‘ Replace with your real API key\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ------------------ Define Tool ------------------\n",
        "def get_weather(city: str) -> str:\n",
        "    # Just a dummy weather function\n",
        "    conditions = [\"sunny â˜€ï¸\", \"rainy ðŸŒ§ï¸\", \"cloudy â˜ï¸\", \"windy ðŸŒ¬ï¸\"]\n",
        "    return f\"The weather in {city} is {random.choice(conditions)}.\"\n",
        "\n",
        "# ------------------ Agent Setup ------------------\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are a friendly weather assistant.\",\n",
        "    model=OpenAIChatCompletionsModel(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        openai_client=client\n",
        "    ),\n",
        "    tools=[get_weather]\n",
        ")\n",
        "\n",
        "# ------------------ Chat Loop ------------------\n",
        "while True:\n",
        "    query = input(\"Enter your question (or 'exit' to quit): \")\n",
        "    if query.lower() == \"exit\":\n",
        "        print(\"Goodbye! ðŸ‘‹\")\n",
        "        break\n",
        "\n",
        "    # Run streamed version (async)\n",
        "    asyncio.get_event_loop().run_until_complete(Runner.run_streamed(agent, query))\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "ObBgtT3_Ue_k",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tool"
      ],
      "metadata": {
        "id": "KK-GOprHrCbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ðŸ§© Example: Weather Fetch Tool with Async Streaming Output ---\n",
        "\n",
        "!pip install openai pydantic asyncio -q\n",
        "\n",
        "import asyncio\n",
        "from pydantic import BaseModel\n",
        "from openai import AsyncOpenAI\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "\n",
        "# --- Initialize client ---\n",
        "client = AsyncOpenAI(api_key=\"YOUR_OPENAI_API_KEY\")\n",
        "\n",
        "# --- Dummy RunContextWrapper class ---\n",
        "class RunContextWrapper:\n",
        "    def __init__(self, data=None):\n",
        "        self.data = data\n",
        "\n",
        "# --- Simulated weather fetching function ---\n",
        "def fetch_weather_data(city: str, unit: str) -> str:\n",
        "    if city.lower() == \"lahore\":\n",
        "        temp = 30 if unit == \"C\" else 86\n",
        "    elif city.lower() == \"karachi\":\n",
        "        temp = 33 if unit == \"C\" else 91\n",
        "    else:\n",
        "        temp = 25 if unit == \"C\" else 77\n",
        "    return f\"The current temperature in {city} is {temp}Â°{unit}.\"\n",
        "\n",
        "# --- Input model for the tool ---\n",
        "class WeatherArgs(BaseModel):\n",
        "    city: str\n",
        "    unit: str\n",
        "\n",
        "# --- Fixed function (removed [Any]) ---\n",
        "async def run_fetch_weather(ctx: RunContextWrapper, args: str) -> str:\n",
        "    parsed = WeatherArgs.model_validate_json(args)\n",
        "    return fetch_weather_data(city=parsed.city, unit=parsed.unit)\n",
        "\n",
        "# --- Async main function ---\n",
        "async def main():\n",
        "    print(\"Type a city name (e.g., Lahore, Karachi, or Islamabad):\")\n",
        "    city = input(\"City: \")\n",
        "    unit = input(\"Unit (C/F): \")\n",
        "\n",
        "    args = WeatherArgs(city=city, unit=unit).model_dump_json()\n",
        "    ctx = RunContextWrapper()\n",
        "\n",
        "    print(\"\\nðŸ¤– Fetching weather info...\\n\")\n",
        "    result_text = await run_fetch_weather(ctx, args)\n",
        "\n",
        "    # Simulate streaming output like ChatGPT\n",
        "    for ch in result_text:\n",
        "        print(ch, end=\"\", flush=True)\n",
        "        await asyncio.sleep(0.03)\n",
        "    print(\"\\n\\nâœ… Done!\")\n",
        "\n",
        "# --- Run the async main loop ---\n",
        "await main()\n"
      ],
      "metadata": {
        "id": "tHO-ciAPrHgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function Tool**"
      ],
      "metadata": {
        "id": "D7blyLB0A7nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai python-dotenv nest_asyncio\n",
        "\n",
        "import os\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "from dotenv import load_dotenv\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow async code inside Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- Load API key ---\n",
        "load_dotenv()\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "if not gemini_api_key:\n",
        "    gemini_api_key = input(\"Enter your Gemini API key: \")\n",
        "\n",
        "# --- Create client ---\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# --- Define fake tools ---\n",
        "async def fetch_weather(location: str) -> str:\n",
        "    return f\"The weather in {location} is 28Â°C and sunny.\"\n",
        "\n",
        "def fetch_news(location: str) -> str:\n",
        "    return f\"Breaking news from {location}: AI revolution continues!\"\n",
        "\n",
        "def fetch_stock_price(company: str) -> str:\n",
        "    return f\"The current stock price of {company} is USD 1000.00.\"\n",
        "\n",
        "# --- Main chat function ---\n",
        "async def main():\n",
        "    print(\"ðŸŒ¦ Welcome to the Gemini Assistant!\")\n",
        "    print(\"Ask about weather, news, or stock prices.\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Enter your query (or 'exit' to quit): \")\n",
        "        if query.lower() == \"exit\":\n",
        "            print(\"Goodbye ðŸ‘‹\")\n",
        "            break\n",
        "\n",
        "        if \"weather\" in query.lower():\n",
        "            location = query.split(\"in\")[-1].strip() if \"in\" in query.lower() else \"your area\"\n",
        "            response = await fetch_weather(location)\n",
        "        elif \"news\" in query.lower():\n",
        "            location = query.split(\"in\")[-1].strip() if \"in\" in query.lower() else \"your area\"\n",
        "            response = fetch_news(location)\n",
        "        elif \"stock\" in query.lower():\n",
        "            company = query.split(\"for\")[-1].strip() if \"for\" in query.lower() else \"Tesla\"\n",
        "            response = fetch_stock_price(company)\n",
        "        else:\n",
        "            completion = await client.chat.completions.create(\n",
        "                model=\"gemini-2.0-flash\",\n",
        "                messages=[{\"role\": \"user\", \"content\": query}],\n",
        "            )\n",
        "            response = completion.choices[0].message.content\n",
        "\n",
        "        print(f\"\\nðŸ¤– {response}\\n\")\n",
        "\n",
        "# --- Run async main() safely in Colab ---\n",
        "await main()\n"
      ],
      "metadata": {
        "id": "VNUytUYWA_-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hosted Tool**"
      ],
      "metadata": {
        "id": "ZhjJ4ieQJ0rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai nest_asyncio python-dotenv -q\n",
        "\n",
        "import nest_asyncio, asyncio, os, time, sys\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow async inside Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ------------------ Basic Agent Classes ------------------\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model, tools=None):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "        self.tools = tools or []\n",
        "\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query):\n",
        "        \"\"\"Runs a chat completion synchronously.\"\"\"\n",
        "        async def _run():\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                {\"role\": \"user\", \"content\": query}\n",
        "            ]\n",
        "            response = await agent.model.client.chat.completions.create(\n",
        "                model=agent.model.model,\n",
        "                messages=messages\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        return asyncio.get_event_loop().run_until_complete(_run())\n",
        "\n",
        "# ------------------ Tool Simulations ------------------\n",
        "class WebSearchTool:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def search(self, query):\n",
        "        # Simulated search tool\n",
        "        return f\"(Simulated) Web search results for: '{query}'\"\n",
        "\n",
        "class FileSearchTool:\n",
        "    def __init__(self, max_num_results=3, vector_store_ids=None):\n",
        "        self.max_num_results = max_num_results\n",
        "        self.vector_store_ids = vector_store_ids or []\n",
        "    def search(self, query):\n",
        "        # Simulated file search\n",
        "        return f\"(Simulated) File search for '{query}' in {self.vector_store_ids}\"\n",
        "\n",
        "# ------------------ Gemini Setup ------------------\n",
        "gemini_api_key = input(\"Enter your Gemini API key: \").strip()\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ------------------ Agent Setup ------------------\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are an expert of agentic AI.\",\n",
        "    model=OpenAIChatCompletionsModel(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        openai_client=client\n",
        "    ),\n",
        "    tools=[\n",
        "        WebSearchTool(),\n",
        "        FileSearchTool(max_num_results=3, vector_store_ids=[\"VECTOR_STORE_ID\"]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ------------------ Run Agent ------------------\n",
        "while True:\n",
        "    query = input(\"\\nEnter your query (or 'exit' to quit): \")\n",
        "    if query.lower() == \"exit\":\n",
        "        print(\"Goodbye! ðŸ‘‹\")\n",
        "        break\n",
        "\n",
        "    # Simulate using tools\n",
        "    print(\"\\nðŸ” Tool outputs:\")\n",
        "    for tool in agent.tools:\n",
        "        if hasattr(tool, \"search\"):\n",
        "            print(\" -\", tool.search(query))\n",
        "\n",
        "    # Get AI response\n",
        "    print(\"\\nðŸ¤– Assistant response:\")\n",
        "    result = Runner.run_sync(agent, query)\n",
        "    for ch in result:\n",
        "        print(ch, end=\"\")\n",
        "        sys.stdout.flush()\n",
        "        time.sleep(0.01)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "YqPTSJL6J3fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!pip install openai python-dotenv --quiet\n",
        "\n",
        "import asyncio\n",
        "from openai import AsyncOpenAI\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# --- Set up your Gemini API key ---\n",
        "GEMINI_API_KEY = input(\"Enter your Gemini API key: \")\n",
        "\n",
        "# --- Create a simple Agent system manually ---\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "\n",
        "    def as_tool(self, tool_name, tool_description):\n",
        "        return {\"tool_name\": tool_name, \"tool_description\": tool_description, \"agent\": self}\n",
        "\n",
        "    async def run(self, input_text):\n",
        "        if \"spanish\" in self.instructions.lower():\n",
        "            return f\"TraducciÃ³n al espaÃ±ol: '{input_text}' â†’ 'Hola, Â¿cÃ³mo estÃ¡s?'\"\n",
        "        elif \"french\" in self.instructions.lower():\n",
        "            return f\"Traduction en franÃ§ais: '{input_text}' â†’ 'Bonjour, comment Ã§a va ?'\"\n",
        "        else:\n",
        "            return f\"{self.name} says: {input_text}\"\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    async def run(agent, input):\n",
        "        return await agent.run(input)\n",
        "\n",
        "# --- Create the agents ---\n",
        "spanish_agent = Agent(\n",
        "    name=\"Spanish agent\",\n",
        "    instructions=\"You translate the user's message to Spanish\",\n",
        ")\n",
        "\n",
        "french_agent = Agent(\n",
        "    name=\"French agent\",\n",
        "    instructions=\"You translate the user's message to French\",\n",
        ")\n",
        "\n",
        "orchestrator_agent = Agent(\n",
        "    name=\"orchestrator_agent\",\n",
        "    instructions=(\n",
        "        \"You are a translation agent. You use the tools given to you to translate.\"\n",
        "        \"If asked for multiple translations, you call the relevant tools.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# --- Simulation: orchestrator chooses the Spanish agent ---\n",
        "async def main():\n",
        "    result = await spanish_agent.run(\"Say 'Hello, how are you?' in french.\")\n",
        "    print(result)\n",
        "\n",
        "# --- Run in Colab safely ---\n",
        "await main()\n"
      ],
      "metadata": {
        "id": "WJW6fX1kKZTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conditional tool enabling**"
      ],
      "metadata": {
        "id": "3qj4Y9eXdBtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio pydantic -q\n",
        "import nest_asyncio, asyncio\n",
        "from pydantic import BaseModel\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# -----------------------\n",
        "#  Simulate Agent Classes\n",
        "# -----------------------\n",
        "\n",
        "class RunContextWrapper:\n",
        "    def __init__(self, context):\n",
        "        self.context = context\n",
        "\n",
        "class AgentBase:\n",
        "    pass\n",
        "\n",
        "class Agent(AgentBase):\n",
        "    def __init__(self, name, instructions):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.tools = []\n",
        "\n",
        "    def as_tool(self, tool_name, tool_description, is_enabled=True):\n",
        "        return {\n",
        "            \"tool_name\": tool_name,\n",
        "            \"tool_description\": tool_description,\n",
        "            \"is_enabled\": is_enabled,\n",
        "            \"agent\": self\n",
        "        }\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    async def run(agent, input_text, context=None):\n",
        "        enabled_tools = []\n",
        "        for tool in agent.tools:\n",
        "            is_enabled = tool[\"is_enabled\"]\n",
        "            if callable(is_enabled):\n",
        "                enabled = is_enabled(RunContextWrapper(context), agent)\n",
        "            else:\n",
        "                enabled = is_enabled\n",
        "            if enabled:\n",
        "                enabled_tools.append(tool)\n",
        "\n",
        "        responses = []\n",
        "        for tool in enabled_tools:\n",
        "            responses.append(f\"{tool['agent'].name} â†’ '{tool['agent'].instructions}' â†’ Responding to: {input_text}\")\n",
        "\n",
        "        return type(\"Result\", (), {\"final_output\": \"\\n\".join(responses)})\n",
        "\n",
        "# -----------------------\n",
        "#  Define Context & Logic\n",
        "# -----------------------\n",
        "\n",
        "class LanguageContext(BaseModel):\n",
        "    language_preference: str = \"french_spanish\"\n",
        "\n",
        "def french_enabled(ctx: RunContextWrapper, agent: AgentBase) -> bool:\n",
        "    return ctx.context.language_preference == \"french_spanish\"\n",
        "\n",
        "# Specialized agents\n",
        "spanish_agent = Agent(\n",
        "    name=\"spanish_agent\",\n",
        "    instructions=\"You respond in Spanish. Always reply to the user's question in Spanish.\"\n",
        ")\n",
        "\n",
        "french_agent = Agent(\n",
        "    name=\"french_agent\",\n",
        "    instructions=\"You respond in French. Always reply to the user's question in French.\"\n",
        ")\n",
        "\n",
        "# Orchestrator agent\n",
        "orchestrator = Agent(\n",
        "    name=\"orchestrator\",\n",
        "    instructions=(\n",
        "        \"You are a multilingual assistant. You use tools to respond in different languages.\"\n",
        "    )\n",
        ")\n",
        "orchestrator.tools = [\n",
        "    spanish_agent.as_tool(\n",
        "        tool_name=\"respond_spanish\",\n",
        "        tool_description=\"Respond to the user's question in Spanish\",\n",
        "        is_enabled=True,\n",
        "    ),\n",
        "    french_agent.as_tool(\n",
        "        tool_name=\"respond_french\",\n",
        "        tool_description=\"Respond to the user's question in French\",\n",
        "        is_enabled=french_enabled,\n",
        "    ),\n",
        "]\n",
        "\n",
        "# -----------------------\n",
        "#  Run in Colab\n",
        "# -----------------------\n",
        "\n",
        "async def main():\n",
        "    context = LanguageContext(language_preference=\"french_spanish\")\n",
        "    result = await Runner.run(orchestrator, \"How are you?\", context=context)\n",
        "    print(\"=== FINAL OUTPUT ===\")\n",
        "    print(result.final_output)\n",
        "\n",
        "await main()\n"
      ],
      "metadata": {
        "id": "Si9nAw9eg-n6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}