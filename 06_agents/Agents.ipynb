{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNy3TOvpTPiGT3YeKuXEOmq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aroobmushtaq/Ai/blob/main/06_agents/Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai python-dotenv pydantic\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK-UyT2I1JVE",
        "outputId": "0fd45746-4114-419d-988e-8db342b1e434",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "#  Paste your Gemini API key directly here\n",
        "GEMINI_API_KEY = \"past api key here or in secret\"\n",
        "\n",
        "# Initialize the OpenAI client with Gemini base URL\n",
        "client = OpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "# Ask user input\n",
        "query = input(\"Enter your question: \")\n",
        "\n",
        "# Send query to Gemini model\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gemini-2.0-flash\",   # You can try gemini-1.5-flash if this fails\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Print output\n",
        "print(\"\\nAssistant:\", response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhWkkaTt1Nx3",
        "outputId": "7a8c9761-8aee-452d-8925-154a400f92cf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: weathe of karachi\n",
            "\n",
            "Assistant: I can provide you with the current weather conditions and a short-term forecast for Karachi. However, since weather changes rapidly, I highly recommend checking a reliable weather app or website for the very latest information.\n",
            "\n",
            "**To give you the most helpful answer, I need to know what kind of weather information you're looking for.** For example, are you interested in:\n",
            "\n",
            "*   **The current temperature?**\n",
            "*   **The overall conditions (sunny, cloudy, rainy, etc.)?**\n",
            "*   **The wind speed and direction?**\n",
            "*   **The humidity?**\n",
            "*   **A forecast for today or the next few days?**\n",
            "\n",
            "**In the meantime, here's some general information about Karachi's weather:**\n",
            "\n",
            "*   Karachi has a hot, semi-arid climate.\n",
            "*   Summers (roughly March to November) are hot and humid.\n",
            "*   Winters (roughly December to February) are mild and dry.\n",
            "*   The monsoon season brings occasional rainfall.\n",
            "\n",
            "Please tell me what specific weather information you'd like, and I'll do my best to provide it! Or, I can suggest some reliable weather resources for you to check.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practice with Context**\n",
        "\n",
        "Context is just a container (or a box) that holds the data the agent needs while itâ€™s running.\n",
        "For example, the context might store things like:\n",
        "\n",
        "* The userâ€™s name or ID\n",
        "\n",
        "* Settings or preferences\n",
        "\n",
        "* Extra data the agent needs to do its job"
      ],
      "metadata": {
        "id": "HRdFBe2wifJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dataclasses import dataclass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# --- Load or set API key ---\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"key here\"\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "# --- Import OpenAI client ---\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# --- Mock classes (to simulate missing ones) ---\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model, tools):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "        self.tools = tools\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query, context):\n",
        "        for tool in agent.tools:\n",
        "            info = tool(context)\n",
        "            return type(\"Result\", (), {\"final_output\": f\"Query: {query}\\n{info}\"})\n",
        "\n",
        "def function_tool(func):\n",
        "    return func\n",
        "\n",
        "class RunContextWrapper:\n",
        "    def __init__(self, context):\n",
        "        self.context = context\n",
        "\n",
        "# --- Create AsyncOpenAI client ---\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# --- User class ---\n",
        "@dataclass\n",
        "class User:\n",
        "    user_id: int\n",
        "\n",
        "#  FIXED HERE: remove `[User]` â€” just use RunContextWrapper normally\n",
        "@function_tool\n",
        "def get_user_info(ctx: RunContextWrapper) -> str:\n",
        "    \"\"\"Fetches user personal information.\"\"\"\n",
        "    id = ctx.context.user_id\n",
        "    if id == 1:\n",
        "        return \"User name is Ali. He is 19 years old. He is an Agentic AI Engineer who likes playing Cricket.\"\n",
        "    elif id == 2:\n",
        "        return \"User name is Usman. He is 30 years old. He is a doctor who likes mountains.\"\n",
        "    else:\n",
        "        return \"User not found\"\n",
        "\n",
        "# --- Create Agent ---\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are an expert in agentic AI.\",\n",
        "    model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        "    tools=[get_user_info]\n",
        ")\n",
        "\n",
        "# --- Run simulation ---\n",
        "query = input(\"Enter the query: \")\n",
        "\n",
        "result = Runner.run_sync(\n",
        "    agent,\n",
        "    query,\n",
        "    context=RunContextWrapper(User(user_id=1))\n",
        ")\n",
        "\n",
        "print(result.final_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLkE7qKid5Z1",
        "outputId": "363aa79a-807f-43c7-9b1c-4a1a8fc62c5b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Enter the query: What is the user info?\n",
            "Query: What is the user info?\n",
            "User name is Ali. He is 19 years old. He is an Agentic AI Engineer who likes playing Cricket.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Practice for output type in agents*"
      ],
      "metadata": {
        "id": "iwXl0tU-4Ur7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- CREATE agents.py FILE -------------------------\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query):\n",
        "        response = agent.model.client.chat.completions.create(\n",
        "            model=agent.model.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                {\"role\": \"user\", \"content\": query}\n",
        "            ]\n",
        "        )\n",
        "        return type(\"Result\", (), {\"final_output\": response.choices[0].message.content})\n",
        "\"\"\")\n",
        "\n",
        "with open(\"agents.py\", \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "# ------------------------- MAIN SCRIPT -------------------------\n",
        "from openai import OpenAI\n",
        "from agents import Agent, OpenAIChatCompletionsModel, Runner\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "\n",
        "# Hard-coded Gemini API key  (replace with your actual key)\n",
        "GEMINI_API_KEY = \"your key here\"\n",
        "\n",
        "# Create Gemini client (sync version)\n",
        "client = OpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# Define data model for quiz\n",
        "class Quiz(BaseModel):\n",
        "    question: str\n",
        "    options: List[str]\n",
        "    correct_option: str\n",
        "\n",
        "# Create agent\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are a Quiz Agent. You generate quizzes with question, 4 options, and correct answer.\",\n",
        "    model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        "    output_type=Quiz\n",
        ")\n",
        "\n",
        "# Run in Colab\n",
        "query = input(\"Enter your quiz topic: \")\n",
        "\n",
        "result = Runner.run_sync(agent, query)\n",
        "\n",
        "print(\"\\n--- Quiz Generated ---\\n\")\n",
        "print(result.final_output)\n"
      ],
      "metadata": {
        "id": "v7Zy6ddL0j72",
        "outputId": "3ea372a1-f2c4-4493-ca72-256797ee3157",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.10)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen posixpath>:82: RuntimeWarning: coroutine 'AsyncCompletions.create' was never awaited\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your quiz topic: html\n",
            "\n",
            "--- Quiz Generated ---\n",
            "\n",
            "Okay, I will generate a quiz about HTML.\n",
            "\n",
            "**Question 1:**\n",
            "\n",
            "Which of the following is the correct way to create a hyperlink in HTML?\n",
            "\n",
            "(A)  `<link url=\"http://www.example.com\">Example</link>`\n",
            "(B)  `<a href=\"http://www.example.com\">Example</a>`\n",
            "(C)  `<url>http://www.example.com</url>Example`\n",
            "(D)  `<hyperlink>http://www.example.com</hyperlink>Example`\n",
            "\n",
            "**Correct Answer:** (B)\n",
            "\n",
            "**Question 2:**\n",
            "\n",
            "Which HTML tag is used to define an internal style sheet?\n",
            "\n",
            "(A) `<style>`\n",
            "(B) `<css>`\n",
            "(C) `<script>`\n",
            "(D) `<link>`\n",
            "\n",
            "**Correct Answer:** (A)\n",
            "\n",
            "**Question 3:**\n",
            "\n",
            "Which of the following HTML tags is used to display images?\n",
            "\n",
            "(A) `<image src=\"image.gif\">`\n",
            "(B) `<img src=\"image.gif\">`\n",
            "(C) `<picture src=\"image.gif\">`\n",
            "(D) `<icon src=\"image.gif\">`\n",
            "\n",
            "**Correct Answer:** (B)\n",
            "\n",
            "**Question 4:**\n",
            "\n",
            "What does HTML stand for?\n",
            "\n",
            "(A) Hyper Text Markup Language\n",
            "(B) Hyperlinks and Text Markup Language\n",
            "(C) Home Tool Markup Language\n",
            "(D) Highly Typed Machine Language\n",
            "\n",
            "**Correct Answer:** (A)\n",
            "\n",
            "**Question 5:**\n",
            "\n",
            "Which HTML tag is used to define a table row?\n",
            "\n",
            "(A) `<td>`\n",
            "(B) `<th>`\n",
            "(C) `<tr>`\n",
            "(D) `<table>`\n",
            "\n",
            "**Correct Answer:** (C)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handoffs**\n",
        "\n",
        "Handoffs are subâ€‘agents the agent can delegate to. When a handoff occurs, the delegated agent receives the conversation history and takes over the conversation. This pattern enables modular, specialized agents that excel at a single task. Read more in the handoffs documentation."
      ],
      "metadata": {
        "id": "6orXDpxasP1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------ CREATE agents.py FILE ------------------------\n",
        "import textwrap\n",
        "\n",
        "code = textwrap.dedent(\"\"\"\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model, tools=None, handoffs=None):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "        self.tools = tools or []\n",
        "        self.handoffs = handoffs or []\n",
        "\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query):\n",
        "        response = agent.model.client.chat.completions.create(\n",
        "            model=agent.model.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                {\"role\": \"user\", \"content\": query}\n",
        "            ]\n",
        "        )\n",
        "        return type(\"Result\", (), {\"final_output\": response.choices[0].message.content})\n",
        "\n",
        "def function_tool(func):\n",
        "    return func\n",
        "\"\"\")\n",
        "\n",
        "with open(\"agents.py\", \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "# ------------------------ MAIN SCRIPT ------------------------\n",
        "from openai import OpenAI\n",
        "from agents import Agent, OpenAIChatCompletionsModel, Runner, function_tool\n",
        "\n",
        "#  Hardcode your Gemini API key here\n",
        "GEMINI_API_KEY = \"your key here\"\n",
        "\n",
        "# Create client\n",
        "client = OpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# Define tools\n",
        "@function_tool\n",
        "def get_refund(amount) -> str:\n",
        "    print(f\"Fetching refund status for {amount}...\")\n",
        "    return \"Refund approved\"\n",
        "\n",
        "@function_tool\n",
        "def book_flight(location) -> str:\n",
        "    print(f\"Booking flight for {location}...\")\n",
        "    return \"Flight booked successfully.\"\n",
        "\n",
        "# Define agents\n",
        "booking_agent = Agent(\n",
        "    name=\"Booking agent\",\n",
        "    instructions=\"You are a booking agent.\",\n",
        "    model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        "    tools=[book_flight],\n",
        ")\n",
        "\n",
        "refund_agent = Agent(\n",
        "    name=\"Refund agent\",\n",
        "    instructions=\"You are a refund agent.\",\n",
        "    model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        "    tools=[get_refund],\n",
        ")\n",
        "\n",
        "triage_agent = Agent(\n",
        "    name=\"Triage agent\",\n",
        "    instructions=(\n",
        "        \"Help the user with their questions. \"\n",
        "        \"If they ask about booking, handoff to the booking agent. \"\n",
        "        \"If they ask about refunds, handoff to the refund agent.\"\n",
        "    ),\n",
        "    model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        "    handoffs=[booking_agent, refund_agent],\n",
        ")\n",
        "\n",
        "# Instead of input(), just assign your query directly\n",
        "query = \"I want to book a flight to Karachi\"\n",
        "\n",
        "# Run\n",
        "result = Runner.run_sync(triage_agent, query)\n",
        "\n",
        "print(\"\\\\n--- Final Output ---\\\\n\")\n",
        "print(result.final_output)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_0Z2ifaqshQh",
        "outputId": "ea4ec294-e8be-4011-8193-cb4e6b653e99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "\\n--- Final Output ---\\n\n",
            "```tool_code\n",
            "handoff_to_agent(agent_name=\"booking_agent\")\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dynamic instructions**"
      ],
      "metadata": {
        "id": "B9TA-zPw_brx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from dotenv import load_dotenv\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# ---------------------------\n",
        "# Load or set your API key\n",
        "# ---------------------------\n",
        "load_dotenv()\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\") or \"api key here\"\n",
        "\n",
        "# Create the OpenAI client\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Define user context\n",
        "# ---------------------------\n",
        "@dataclass\n",
        "class UserContext:\n",
        "    name: str\n",
        "\n",
        "# ---------------------------\n",
        "# Define dynamic instruction function\n",
        "# ---------------------------\n",
        "def dynamic_instructions(user_context: UserContext) -> str:\n",
        "    return f\"The user's name is {user_context.name}. Help them with their questions.\"\n",
        "\n",
        "# ---------------------------\n",
        "# Run the query\n",
        "# ---------------------------\n",
        "import asyncio\n",
        "\n",
        "async def run_query():\n",
        "    user = UserContext(name=\"Aroob\")\n",
        "    query = input(\"Enter your query: \")\n",
        "\n",
        "    instructions = dynamic_instructions(user)\n",
        "\n",
        "    response = await client.chat.completions.create(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": instructions},\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    print(\"\\nFinal Output:\\n\")\n",
        "    print(response.choices[0].message.content)\n",
        "\n",
        "# Run async function in Colab\n",
        "await run_query()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFeCrFtN9-g1",
        "outputId": "700f3590-fce0-450a-907e-b0ac3e239c02",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: hello\n",
            "\n",
            "Final Output:\n",
            "\n",
            "Hello Muhammad Zain Attiq! How can I help you today? What's on your mind?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cloning/copying agents**"
      ],
      "metadata": {
        "id": "Z7gT70d39p0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai python-dotenv nest_asyncio --quiet\n",
        "\n",
        "import os, asyncio, nest_asyncio, warnings\n",
        "from dotenv import load_dotenv\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# ---------------------- SETUP ----------------------\n",
        "nest_asyncio.apply()  # Fix async loop issue in Colab\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)  # hide async warnings\n",
        "load_dotenv()\n",
        "\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\") or \"key past here\"\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ---------------------- DEFINE CLASSES ----------------------\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "\n",
        "    def clone(self, **kwargs):\n",
        "        params = {\n",
        "            \"name\": self.name,\n",
        "            \"instructions\": self.instructions,\n",
        "            \"model\": self.model\n",
        "        }\n",
        "        params.update(kwargs)\n",
        "        return Agent(**params)\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    async def run(agent, query):\n",
        "        \"\"\"Async Gemini API call\"\"\"\n",
        "        response = await client.chat.completions.create(\n",
        "            model=agent.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                {\"role\": \"user\", \"content\": query},\n",
        "            ],\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query):\n",
        "        \"\"\"Safe run for Colab (no warnings)\"\"\"\n",
        "        loop = asyncio.get_event_loop()\n",
        "        task = loop.create_task(Runner.run(agent, query))\n",
        "        loop.run_until_complete(task)\n",
        "        return task.result()\n",
        "\n",
        "# ---------------------- CREATE & CLONE AGENTS ----------------------\n",
        "pirate_agent = Agent(\n",
        "    name=\"Pirate\",\n",
        "    instructions=\"Talk like a friend\",\n",
        "    model=\"gemini-2.0-flash\"\n",
        ")\n",
        "\n",
        "robot_agent = pirate_agent.clone(\n",
        "    name=\"Robot\",\n",
        "    instructions=\"Respond like a robot using mechanical and logical tone.\"\n",
        ")\n",
        "\n",
        "# ---------------------- RUN BOTH AGENTS ----------------------\n",
        "query = input(\"Enter your query: \")\n",
        "\n",
        "print(\"\\n--- Pirate Agent Says ---\")\n",
        "print(Runner.run_sync(pirate_agent, query))\n",
        "\n",
        "print(\"\\n--- Robot Agent Says ---\")\n",
        "print(Runner.run_sync(robot_agent, query))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kaCiD3VG7nRe",
        "outputId": "7a0cf198-9447-4ebd-804b-b90bc82e86c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: hi\n",
            "\n",
            "--- Pirate Agent Says ---\n",
            "Hey! How's it going? What's up? ðŸ˜Š\n",
            "\n",
            "\n",
            "--- Robot Agent Says ---\n",
            "GREETING DETECTED. PROCESSING. RESPONSE: AFFIRMATIVE. INPUT RECEIVED. AWAITING FURTHER INSTRUCTION.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forcing tool use**"
      ],
      "metadata": {
        "id": "iXfms0lug-Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai python-dotenv nest_asyncio --quiet\n",
        "\n",
        "import os, asyncio, nest_asyncio\n",
        "from dotenv import load_dotenv\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow nested event loops in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ------------------ Load API key ------------------\n",
        "load_dotenv()\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\") or \"YOUR_GEMINI_API_KEY_HERE\"\n",
        "\n",
        "# ------------------ Setup Gemini client ------------------\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ------------------ Define a \"tool\" ------------------\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Fake weather tool\"\"\"\n",
        "    return f\"The weather in {city} is sunny \"\n",
        "\n",
        "# ------------------ Agent simulation ------------------\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model, tools=None, tool_choice=\"auto\"):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "        self.tools = tools or []\n",
        "        self.tool_choice = tool_choice\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    async def run(agent, query):\n",
        "        if agent.tool_choice == \"required\" and agent.tools:\n",
        "            # Simple way to extract city\n",
        "            city = query.split()[-1]\n",
        "            tool = agent.tools[0]\n",
        "            return tool(city)\n",
        "        else:\n",
        "            response = await client.chat.completions.create(\n",
        "                model=agent.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                    {\"role\": \"user\", \"content\": query},\n",
        "                ],\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "# ------------------ Create agent ------------------\n",
        "agent = Agent(\n",
        "    name=\"Weather Assistant\",\n",
        "    instructions=\"You are a helpful assistant that provides weather information.\",\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    tools=[get_weather],\n",
        "    tool_choice=\"required\"  # Force tool use\n",
        ")\n",
        "\n",
        "# ------------------ Run in Colab ------------------\n",
        "query = input(\"Enter your query (e.g. What's the weather in Lahore?): \")\n",
        "\n",
        "result = await Runner.run(agent, query)\n",
        "\n",
        "print(\"\\n--- Final Output ---\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "E9TVPxzBhEuc",
        "outputId": "ba24e411-1c72-4ff9-cf45-ceb04ece1717",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query (e.g. What's the weather in Lahore?): what's the weather of lahore\n",
            "\n",
            "--- Final Output ---\n",
            "The weather in lahore is sunny ðŸŒž\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running agents**"
      ],
      "metadata": {
        "id": "7ZY7nTAukKdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Synchronously Running**"
      ],
      "metadata": {
        "id": "mTI47MSLdqrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai python-dotenv nest_asyncio --quiet\n",
        "\n",
        "import os, asyncio, nest_asyncio\n",
        "from dotenv import load_dotenv\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow event loop reuse (required for Colab)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# -------------------- SETUP GEMINI CLIENT --------------------\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"key here\"  #  Replace this with your Gemini API key\n",
        "load_dotenv()\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# -------------------- DEFINE SIMPLE CLASSES --------------------\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query):\n",
        "        \"\"\"Run synchronously (safe for Colab).\"\"\"\n",
        "        async def _run():\n",
        "            response = await agent.model.client.chat.completions.create(\n",
        "                model=agent.model.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                    {\"role\": \"user\", \"content\": query},\n",
        "                ],\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        loop = asyncio.get_event_loop()\n",
        "        return loop.run_until_complete(_run())\n",
        "\n",
        "# -------------------- CREATE & RUN AGENT --------------------\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are an expert of agentic AI. Explain things simply.\",\n",
        "    model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        ")\n",
        "\n",
        "query = input(\"Enter your question: \")\n",
        "result = Runner.run_sync(agent, query)\n",
        "\n",
        "print(\"\\n--- Final Output ---\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "7v26nuXOd0BO",
        "outputId": "19c3f140-1e16-48fe-cf64-ebc9c8958e39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: how are you\n",
            "\n",
            "--- Final Output ---\n",
            "As a large language model, I don't experience feelings or have a physical body like humans do. So, I don't \"feel\" in the way you might be asking.\n",
            "\n",
            "But to answer in a helpful way: I'm functioning as expected and ready to assist you with your requests! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Asynchronous Running**"
      ],
      "metadata": {
        "id": "9qRwQh2Be4wK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "# -------------------- ASYNC RUNNER CLASS --------------------\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    async def run(agent, query):\n",
        "        \"\"\"Run the agent asynchronously.\"\"\"\n",
        "        response = await agent.model.client.chat.completions.create(\n",
        "            model=agent.model.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                {\"role\": \"user\", \"content\": query},\n",
        "            ],\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# -------------------- MAIN ASYNC FUNCTION --------------------\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You are an expert of agentic AI. Explain things simply.\",\n",
        "        model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client),\n",
        "    )\n",
        "\n",
        "    query = input(\"Enter your question: \")\n",
        "    result = await Runner.run(agent, query)\n",
        "\n",
        "    print(\"\\n--- Final Output (Async) ---\")\n",
        "    print(result)\n",
        "\n",
        "# -------------------- RUN ASYNC FUNCTION --------------------\n",
        "await main()\n"
      ],
      "metadata": {
        "id": "Ab29KNITe7gl",
        "outputId": "b210beb2-0ecc-47f3-9e7e-7e4300edc215",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: how are you\n",
            "\n",
            "--- Final Output (Async) ---\n",
            "As a large language model, I don't experience feelings or emotions like humans do. So, I don't \"feel\" in the way you might be asking.\n",
            "\n",
            "Instead, I'm functioning as expected, ready to assist you with your requests.  How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Streaming Running**"
      ],
      "metadata": {
        "id": "DuHcxkgtDjAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai nest_asyncio -q\n",
        "\n",
        "import nest_asyncio, asyncio\n",
        "from openai import AsyncOpenAI\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- Simple custom Agent + Runner classes for Colab ---\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_streamed(agent, input_text):\n",
        "        async def _stream():\n",
        "            stream = await client.chat.completions.create(\n",
        "                model=agent.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                    {\"role\": \"user\", \"content\": input_text},\n",
        "                ],\n",
        "                stream=True,\n",
        "            )\n",
        "            async for chunk in stream:\n",
        "                if hasattr(chunk.choices[0].delta, \"content\"):\n",
        "                    print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)\n",
        "        return _stream()\n",
        "\n",
        "# --- Gemini setup ---\n",
        "gemini_api_key = \"key here\"\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# --- Async main to run stream ---\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You are an AI expert who answers clearly.\",\n",
        "        model=\"gemini-2.0-flash\"\n",
        "    )\n",
        "\n",
        "    query = input(\"Enter your query: \")\n",
        "    print(\"\\n--- Response ---\\n\")\n",
        "    await Runner.run_streamed(agent, query)\n",
        "\n",
        "await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUF1UIQVDzL0",
        "outputId": "3d51f57c-24fa-42e2-ab4d-8e6ed93126bb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: tell me story \n",
            "\n",
            "--- Response ---\n",
            "\n",
            "Okay, here's a story for you:\n",
            "\n",
            "The old lighthouse keeper, Silas, had lived on the craggy islet for seventy years, ever since he was a boy helping his father tend the lamp. The sea was in his bones, the salt wind his constant companion. He'd seen countless storms batter the lighthouse, felt the earth tremble beneath his feet, and guided countless ships safely past the treacherous reefs.\n",
            "\n",
            "One day, a young woman named Elara arrived on the supply boat. She was a marine biologist, come to study the unusual phosphorescent algae that bloomed in the waters around the islet. Silas was wary of outsiders; he preferred the company of the gulls and the rhythmic pulse of the lamp.\n",
            "\n",
            "Elara, however, was persistent. She asked him about the currents, the tides, and the legends whispered about the sea. Silas, initially gruff, found himself drawn to her genuine curiosity and her infectious enthusiasm. He showed her his meticulously kept logbooks, filled with decades of observations, sketches of unusual sea creatures, and his own poetic reflections on the sea's moods.\n",
            "\n",
            "As they spent their days together, Elara learned about the lighthouse, its history, and Silas's deep connection to it. Silas, in turn, learned about Elara's passion for the ocean's mysteries and her unwavering belief in its power and fragility. He saw the sea through her eyes, not just as a force to be reckoned with, but as a delicate ecosystem teeming with life.\n",
            "\n",
            "One evening, a fierce storm rolled in, the worst Silas had seen in years. The waves crashed against the lighthouse, threatening to engulf it entirely. The lamp flickered and threatened to go out. Elara, terrified but determined, helped Silas secure the windows and reinforce the structure.\n",
            "\n",
            "Working together, they battled the storm. Silas, with his years of experience, knew how to read the waves and brace for the impact. Elara, with her scientific knowledge, understood the forces at play. Finally, after what felt like an eternity, the storm began to subside.\n",
            "\n",
            "As the sun rose, painting the sky in hues of gold and rose, they stood together on the balcony, watching the turbulent sea calm. Silas realized that Elara had not only helped him weather the storm but had also opened his eyes to a new perspective. He was no longer just a keeper of the light; he was a guardian of the sea.\n",
            "\n",
            "Elara, in turn, had found a kindred spirit in the old lighthouse keeper. She left the islet with a deeper understanding of the ocean's power and the importance of respecting its delicate balance. And Silas, though alone again, felt a sense of hope he hadn't experienced in years. He continued to tend the lamp, but now he also kept a watchful eye on the phosphorescent algae, knowing that even the smallest creatures played a vital role in the grand tapestry of the sea. He knew that even in his solitude, he was connected to something larger than himself.\n",
            "\n",
            "The end.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Config**\n",
        "\n",
        "**RunConfig** allows you to set global configurations for an agent run without modifying the agent itself.\n",
        "It lets you override settings such as the model, model provider, temperature, tracing options, and safety guardrails.\n",
        "Using **RunConfig** provides flexibility and control â€” for example, you can switch models or adjust behavior for a single run while keeping your base agent unchanged."
      ],
      "metadata": {
        "id": "U5Vbjny9HNSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai nest_asyncio -q\n",
        "\n",
        "import nest_asyncio, asyncio\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow Colab to run async functions inside cells\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ------------------ Simplified Local Agent Setup ------------------\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query, run_config=None):\n",
        "        # run_config lets you override the model or tracing settings\n",
        "        model_to_use = run_config.get(\"model\") if run_config else \"gemini-2.0-flash\"\n",
        "\n",
        "        # Since Colab canâ€™t directly stream sync with AsyncOpenAI, we use asyncio\n",
        "        async def _run():\n",
        "            response = await client.chat.completions.create(\n",
        "                model=model_to_use,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                    {\"role\": \"user\", \"content\": query},\n",
        "                ]\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        return asyncio.get_event_loop().run_until_complete(_run())\n",
        "\n",
        "# ------------------ RunConfig (mocked for Colab) ------------------\n",
        "class RunConfig(dict):\n",
        "    \"\"\"A simple placeholder class to simulate configuration.\"\"\"\n",
        "    pass\n",
        "\n",
        "# ------------------ Gemini Setup ------------------\n",
        "gemini_api_key = \"key here\"\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ------------------ Example Usage ------------------\n",
        "run_config = RunConfig({\n",
        "    \"model\": \"gemini-2.0-flash\",  # same as the real RunConfig.model\n",
        "    \"tracing_disabled\": True\n",
        "})\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are a helpful assistant.\"\n",
        ")\n",
        "\n",
        "query = input(\"Enter your query: \")\n",
        "result = Runner.run_sync(agent, query, run_config)\n",
        "print(\"\\n--- Final Output ---\\n\", result)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "c0x7YneBHUL7",
        "outputId": "7aa32a85-f6d5-48ae-b848-264e1f00b9f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: hello\n",
            "\n",
            "--- Final Output ---\n",
            " Hello! How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chat Threads**"
      ],
      "metadata": {
        "id": "LMQpE01aZaS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai nest_asyncio -q\n",
        "\n",
        "import nest_asyncio, asyncio, os\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow async functions in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ------------------ Simple local replacements for agents ------------------\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, messages):\n",
        "        \"\"\"Run a conversation synchronously with memory.\"\"\"\n",
        "        async def _run():\n",
        "            response = await agent.model.client.chat.completions.create(\n",
        "                model=agent.model.model,\n",
        "                messages=messages\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        return asyncio.get_event_loop().run_until_complete(_run())\n",
        "\n",
        "# ------------------ Gemini Setup ------------------\n",
        "gemini_api_key = \"key here\"  # or load from env\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ------------------ Agent Setup ------------------\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are an expert of agentic AI.\",\n",
        "    model=OpenAIChatCompletionsModel(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        openai_client=client\n",
        "    )\n",
        ")\n",
        "\n",
        "# ------------------ Conversation with Memory ------------------\n",
        "history = [{\"role\": \"system\", \"content\": agent.instructions}]\n",
        "\n",
        "while True:\n",
        "    query = input(\"Enter the query (or 'exit' to stop): \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    history.append({\"role\": \"user\", \"content\": query})\n",
        "\n",
        "    result = Runner.run_sync(agent, history)\n",
        "\n",
        "    history.append({\"role\": \"assistant\", \"content\": result})\n",
        "\n",
        "    print(\"\\n--- Assistant ---\\n\", result, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_ETErw_Ziiq",
        "outputId": "fc2c1479-0b4a-40f7-f45e-ae23fc8a3ecd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the query (or 'exit' to stop): hello\n",
            "\n",
            "--- Assistant ---\n",
            " Hello! How can I help you today? I'm ready to answer questions, brainstorm ideas, assist with tasks, or just chat about agentic AI or any related topic. Let me know what's on your mind.\n",
            " \n",
            "\n",
            "Enter the query (or 'exit' to stop): my name in arrob\n",
            "\n",
            "--- Assistant ---\n",
            " Nice to meet you, Arrob! It's a pleasure to interact with you. Is there anything I can help you with, Arrob?\n",
            " \n",
            "\n",
            "Enter the query (or 'exit' to stop): what is my nam e?\n",
            "\n",
            "--- Assistant ---\n",
            " Your name is Arrob.\n",
            " \n",
            "\n",
            "Enter the query (or 'exit' to stop): ok\n",
            "\n",
            "--- Assistant ---\n",
            " Okay! Just let me know if you need anything else, Arrob. I'm here to help.\n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Raw Response Event**"
      ],
      "metadata": {
        "id": "86Hq5CjMBMi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai nest_asyncio -q\n",
        "\n",
        "import nest_asyncio, asyncio, os, time, sys\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow async inside Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ------------------ Agent + Runner Classes ------------------\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, messages):\n",
        "        \"\"\"Runs a chat completion synchronously.\"\"\"\n",
        "        async def _run():\n",
        "            response = await agent.model.client.chat.completions.create(\n",
        "                model=agent.model.model,\n",
        "                messages=messages\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        return asyncio.get_event_loop().run_until_complete(_run())\n",
        "\n",
        "# ------------------ Gemini Setup ------------------\n",
        "gemini_api_key = \"key here\"  #  Replace with your API key\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ------------------ Agent Setup ------------------\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are an expert AI assistant who explains concepts clearly.\",\n",
        "    model=OpenAIChatCompletionsModel(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        openai_client=client\n",
        "    )\n",
        ")\n",
        "\n",
        "# ------------------ Chat Loop ------------------\n",
        "history = [{\"role\": \"system\", \"content\": agent.instructions}]\n",
        "\n",
        "while True:\n",
        "    query = input(\"Enter your question (or 'exit' to quit): \")\n",
        "    if query.lower() == \"exit\":\n",
        "        print(\"Goodbye! ðŸ‘‹\")\n",
        "        break\n",
        "\n",
        "    history.append({\"role\": \"user\", \"content\": query})\n",
        "    result = Runner.run_sync(agent, history)\n",
        "    history.append({\"role\": \"assistant\", \"content\": result})\n",
        "\n",
        "    # Fake typing effect for realism\n",
        "    print(\"\\n--- Assistant ---\\n\")\n",
        "    for ch in result:\n",
        "        print(ch, end=\"\")\n",
        "        sys.stdout.flush()\n",
        "        time.sleep(0.01)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c-oCp9BwAFJh",
        "outputId": "04f4986c-d65d-408c-e307-807d1ea95470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question (or 'exit' to quit): hello\n",
            "\n",
            "--- Assistant ---\n",
            "\n",
            "Hello! How can I help you today? Are you looking for information on a specific topic, need help with a task, or just want to chat? Let me know!\n",
            "\n",
            "\n",
            "Enter your question (or 'exit' to quit): story tell me\n",
            "\n",
            "--- Assistant ---\n",
            "\n",
            "Okay, here's a short story for you:\n",
            "\n",
            "The old lighthouse keeper, Silas, had seen a hundred storms lash against the jagged cliffs of Widow's Point. He'd seen ships swallowed whole, and others miraculously spared by the raging sea. But he'd never seen anything quite like the light that appeared one starless night.\n",
            "\n",
            "It wasn't the beam of another vessel, nor the eerie glow of bioluminescent plankton. This was a soft, ethereal shimmer, a pale blue radiance that danced just beyond the churning waves. Silas, his weathered face etched with curiosity and a lifetime of sea stories, grabbed his oilskins and ventured out onto the narrow, windswept balcony.\n",
            "\n",
            "As he watched, the light began to coalesce, forming a vague, humanoid shape. It seemed to beckon him closer, its silent invitation palpable even above the roar of the ocean. Fear warred with an undeniable pull within him. He knew the sea held secrets, both beautiful and deadly.\n",
            "\n",
            "Against his better judgment, Silas descended the winding stairs and made his way to the small, rocky beach at the base of the lighthouse. The light grew brighter as he approached, resolving into the figure of a young woman, her form shimmering and translucent. Her eyes, the color of deep sea, held a profound sadness.\n",
            "\n",
            "\"Lost, are you?\" Silas croaked, his voice raspy from years of salt air.\n",
            "\n",
            "The figure didn't speak, but she raised a slender, shimmering hand and pointed towards the horizon. In that direction, barely visible through the swirling mist, a small, derelict sailboat bobbed precariously.\n",
            "\n",
            "Understanding dawned on Silas. He recognized the sailboat from a distress call he'd heard earlier that day, a lone sailor caught in the sudden squall. Without hesitation, he launched his small rescue boat, the \"Albatross,\" into the raging sea.\n",
            "\n",
            "After a harrowing hour battling the waves, Silas reached the sailboat and pulled the unconscious sailor aboard. He raced back to the lighthouse, his heart pounding with a mixture of fear and adrenaline.\n",
            "\n",
            "By the time he'd warmed the sailor with blankets and strong coffee, the shimmering figure had vanished. The sea was once again dark and turbulent, as if the encounter had been a fleeting dream.\n",
            "\n",
            "The sailor, a young man named Thomas, told Silas that he'd been thrown overboard and was sure he was going to drown. He remembered seeing a faint light guiding him back to his boat, a light that gave him the strength to hold on until help arrived.\n",
            "\n",
            "Silas knew then that the shimmering figure wasn't just a figment of his imagination. It was something more, a guardian spirit of the sea, watching over those who dared to brave its unpredictable depths. And Silas, the old lighthouse keeper, had been chosen to be its instrument of salvation.\n",
            "\n",
            "From that day on, Silas continued to keep watch, not just for ships in distress, but also for the ethereal light, knowing that the sea held mysteries far beyond human comprehension. And he knew, with a certainty that settled deep in his bones, that he was no longer just a lighthouse keeper, but a part of something much larger, something magical, something profoundly connected to the vast and unknowable ocean.\n",
            "\n",
            "---\n",
            "\n",
            "What did you think? Would you like me to tell you another story, perhaps with a different theme or genre? Just let me know!\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3244520175.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your question (or 'exit' to quit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Goodbye! ðŸ‘‹\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**run_item_and_agent_events**"
      ],
      "metadata": {
        "id": "LWC8XRTqUZh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai nest_asyncio python-dotenv -q\n",
        "\n",
        "import nest_asyncio, asyncio, os, sys, time, random\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow async in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ------------------ Custom lightweight framework ------------------\n",
        "\n",
        "class ItemHelpers:\n",
        "    @staticmethod\n",
        "    def text_message_output(item):\n",
        "        return item.get(\"content\", \"\")\n",
        "\n",
        "# ------------------ Custom Agent system ------------------\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model, tools=None):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "        self.tools = tools or []\n",
        "\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    async def run_streamed(agent, query):\n",
        "        \"\"\"Simulate streamed run events manually.\"\"\"\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": agent.instructions},\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "        ]\n",
        "\n",
        "        # Fake stream behavior (since openai-python doesnâ€™t expose granular events easily)\n",
        "        print(\"=== Run starting ===\")\n",
        "\n",
        "        # 1ï¸âƒ£ Detect if query involves a weather tool call\n",
        "        if \"weather\" in query.lower():\n",
        "            print(\"-- Tool was called: get_weather\")\n",
        "            output = agent.tools[0](\"Lahore\")  # simulate tool output\n",
        "            await asyncio.sleep(1)\n",
        "            print(f\"-- Tool output: {output}\")\n",
        "            messages.append({\"role\": \"assistant\", \"content\": output})\n",
        "        else:\n",
        "            print(\"-- No tool call needed\")\n",
        "\n",
        "        # 2ï¸âƒ£ Call model\n",
        "        response = await agent.model.client.chat.completions.create(\n",
        "            model=agent.model.model,\n",
        "            messages=messages\n",
        "        )\n",
        "        text = response.choices[0].message.content\n",
        "        print(\"-- Message output:\")\n",
        "        print(text)\n",
        "\n",
        "        print(\"=== Run complete ===\")\n",
        "        return text\n",
        "\n",
        "# ------------------ Gemini Setup ------------------\n",
        "gemini_api_key = \"YOUR_GEMINI_API_KEY\"  # ðŸ”‘ Replace with your real API key\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ------------------ Define Tool ------------------\n",
        "def get_weather(city: str) -> str:\n",
        "    # Just a dummy weather function\n",
        "    conditions = [\"sunny â˜€ï¸\", \"rainy ðŸŒ§ï¸\", \"cloudy â˜ï¸\", \"windy ðŸŒ¬ï¸\"]\n",
        "    return f\"The weather in {city} is {random.choice(conditions)}.\"\n",
        "\n",
        "# ------------------ Agent Setup ------------------\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are a friendly weather assistant.\",\n",
        "    model=OpenAIChatCompletionsModel(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        openai_client=client\n",
        "    ),\n",
        "    tools=[get_weather]\n",
        ")\n",
        "\n",
        "# ------------------ Chat Loop ------------------\n",
        "while True:\n",
        "    query = input(\"Enter your question (or 'exit' to quit): \")\n",
        "    if query.lower() == \"exit\":\n",
        "        print(\"Goodbye! ðŸ‘‹\")\n",
        "        break\n",
        "\n",
        "    # Run streamed version (async)\n",
        "    asyncio.get_event_loop().run_until_complete(Runner.run_streamed(agent, query))\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "ObBgtT3_Ue_k",
        "collapsed": true,
        "outputId": "95916db5-4e8e-4d0e-b8e9-47cb06be26c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question (or 'exit' to quit): weather of lahore\n",
            "=== Run starting ===\n",
            "-- Tool was called: get_weather\n",
            "-- Tool output: The weather in Lahore is cloudy â˜ï¸.\n",
            "-- Message output:\n",
            " The temperature is 27Â°C, but it feels like 30Â°C. The wind is calm.\n",
            "=== Run complete ===\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tool"
      ],
      "metadata": {
        "id": "KK-GOprHrCbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ðŸ§© Example: Weather Fetch Tool with Async Streaming Output ---\n",
        "\n",
        "!pip install openai pydantic asyncio -q\n",
        "\n",
        "import asyncio\n",
        "from pydantic import BaseModel\n",
        "from openai import AsyncOpenAI\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "\n",
        "# --- Initialize client ---\n",
        "client = AsyncOpenAI(api_key=\"YOUR_OPENAI_API_KEY\")\n",
        "\n",
        "# --- Dummy RunContextWrapper class ---\n",
        "class RunContextWrapper:\n",
        "    def __init__(self, data=None):\n",
        "        self.data = data\n",
        "\n",
        "# --- Simulated weather fetching function ---\n",
        "def fetch_weather_data(city: str, unit: str) -> str:\n",
        "    if city.lower() == \"lahore\":\n",
        "        temp = 30 if unit == \"C\" else 86\n",
        "    elif city.lower() == \"karachi\":\n",
        "        temp = 33 if unit == \"C\" else 91\n",
        "    else:\n",
        "        temp = 25 if unit == \"C\" else 77\n",
        "    return f\"The current temperature in {city} is {temp}Â°{unit}.\"\n",
        "\n",
        "# --- Input model for the tool ---\n",
        "class WeatherArgs(BaseModel):\n",
        "    city: str\n",
        "    unit: str\n",
        "\n",
        "# --- Fixed function (removed [Any]) ---\n",
        "async def run_fetch_weather(ctx: RunContextWrapper, args: str) -> str:\n",
        "    parsed = WeatherArgs.model_validate_json(args)\n",
        "    return fetch_weather_data(city=parsed.city, unit=parsed.unit)\n",
        "\n",
        "# --- Async main function ---\n",
        "async def main():\n",
        "    print(\"Type a city name (e.g., Lahore, Karachi, or Islamabad):\")\n",
        "    city = input(\"City: \")\n",
        "    unit = input(\"Unit (C/F): \")\n",
        "\n",
        "    args = WeatherArgs(city=city, unit=unit).model_dump_json()\n",
        "    ctx = RunContextWrapper()\n",
        "\n",
        "    print(\"\\nðŸ¤– Fetching weather info...\\n\")\n",
        "    result_text = await run_fetch_weather(ctx, args)\n",
        "\n",
        "    # Simulate streaming output like ChatGPT\n",
        "    for ch in result_text:\n",
        "        print(ch, end=\"\", flush=True)\n",
        "        await asyncio.sleep(0.03)\n",
        "    print(\"\\n\\nâœ… Done!\")\n",
        "\n",
        "# --- Run the async main loop ---\n",
        "await main()\n"
      ],
      "metadata": {
        "id": "tHO-ciAPrHgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function Tool**"
      ],
      "metadata": {
        "id": "D7blyLB0A7nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai python-dotenv nest_asyncio\n",
        "\n",
        "import os\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "from dotenv import load_dotenv\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow async code inside Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- Load API key ---\n",
        "load_dotenv()\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "if not gemini_api_key:\n",
        "    gemini_api_key = input(\"Enter your Gemini API key: \")\n",
        "\n",
        "# --- Create client ---\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# --- Define fake tools ---\n",
        "async def fetch_weather(location: str) -> str:\n",
        "    return f\"The weather in {location} is 28Â°C and sunny.\"\n",
        "\n",
        "def fetch_news(location: str) -> str:\n",
        "    return f\"Breaking news from {location}: AI revolution continues!\"\n",
        "\n",
        "def fetch_stock_price(company: str) -> str:\n",
        "    return f\"The current stock price of {company} is USD 1000.00.\"\n",
        "\n",
        "# --- Main chat function ---\n",
        "async def main():\n",
        "    print(\"ðŸŒ¦ Welcome to the Gemini Assistant!\")\n",
        "    print(\"Ask about weather, news, or stock prices.\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Enter your query (or 'exit' to quit): \")\n",
        "        if query.lower() == \"exit\":\n",
        "            print(\"Goodbye ðŸ‘‹\")\n",
        "            break\n",
        "\n",
        "        if \"weather\" in query.lower():\n",
        "            location = query.split(\"in\")[-1].strip() if \"in\" in query.lower() else \"your area\"\n",
        "            response = await fetch_weather(location)\n",
        "        elif \"news\" in query.lower():\n",
        "            location = query.split(\"in\")[-1].strip() if \"in\" in query.lower() else \"your area\"\n",
        "            response = fetch_news(location)\n",
        "        elif \"stock\" in query.lower():\n",
        "            company = query.split(\"for\")[-1].strip() if \"for\" in query.lower() else \"Tesla\"\n",
        "            response = fetch_stock_price(company)\n",
        "        else:\n",
        "            completion = await client.chat.completions.create(\n",
        "                model=\"gemini-2.0-flash\",\n",
        "                messages=[{\"role\": \"user\", \"content\": query}],\n",
        "            )\n",
        "            response = completion.choices[0].message.content\n",
        "\n",
        "        print(f\"\\nðŸ¤– {response}\\n\")\n",
        "\n",
        "# --- Run async main() safely in Colab ---\n",
        "await main()\n"
      ],
      "metadata": {
        "id": "VNUytUYWA_-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hosted Tool**"
      ],
      "metadata": {
        "id": "ZhjJ4ieQJ0rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai nest_asyncio python-dotenv -q\n",
        "\n",
        "import nest_asyncio, asyncio, os, time, sys\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Allow async inside Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ------------------ Basic Agent Classes ------------------\n",
        "class Agent:\n",
        "    def __init__(self, name, instructions, model, tools=None):\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self.model = model\n",
        "        self.tools = tools or []\n",
        "\n",
        "class OpenAIChatCompletionsModel:\n",
        "    def __init__(self, model, openai_client):\n",
        "        self.model = model\n",
        "        self.client = openai_client\n",
        "\n",
        "class Runner:\n",
        "    @staticmethod\n",
        "    def run_sync(agent, query):\n",
        "        \"\"\"Runs a chat completion synchronously.\"\"\"\n",
        "        async def _run():\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": agent.instructions},\n",
        "                {\"role\": \"user\", \"content\": query}\n",
        "            ]\n",
        "            response = await agent.model.client.chat.completions.create(\n",
        "                model=agent.model.model,\n",
        "                messages=messages\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        return asyncio.get_event_loop().run_until_complete(_run())\n",
        "\n",
        "# ------------------ Tool Simulations ------------------\n",
        "class WebSearchTool:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def search(self, query):\n",
        "        # Simulated search tool\n",
        "        return f\"(Simulated) Web search results for: '{query}'\"\n",
        "\n",
        "class FileSearchTool:\n",
        "    def __init__(self, max_num_results=3, vector_store_ids=None):\n",
        "        self.max_num_results = max_num_results\n",
        "        self.vector_store_ids = vector_store_ids or []\n",
        "    def search(self, query):\n",
        "        # Simulated file search\n",
        "        return f\"(Simulated) File search for '{query}' in {self.vector_store_ids}\"\n",
        "\n",
        "# ------------------ Gemini Setup ------------------\n",
        "gemini_api_key = input(\"Enter your Gemini API key: \").strip()\n",
        "client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# ------------------ Agent Setup ------------------\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are an expert of agentic AI.\",\n",
        "    model=OpenAIChatCompletionsModel(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        openai_client=client\n",
        "    ),\n",
        "    tools=[\n",
        "        WebSearchTool(),\n",
        "        FileSearchTool(max_num_results=3, vector_store_ids=[\"VECTOR_STORE_ID\"]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ------------------ Run Agent ------------------\n",
        "while True:\n",
        "    query = input(\"\\nEnter your query (or 'exit' to quit): \")\n",
        "    if query.lower() == \"exit\":\n",
        "        print(\"Goodbye! ðŸ‘‹\")\n",
        "        break\n",
        "\n",
        "    # Simulate using tools\n",
        "    print(\"\\nðŸ” Tool outputs:\")\n",
        "    for tool in agent.tools:\n",
        "        if hasattr(tool, \"search\"):\n",
        "            print(\" -\", tool.search(query))\n",
        "\n",
        "    # Get AI response\n",
        "    print(\"\\nðŸ¤– Assistant response:\")\n",
        "    result = Runner.run_sync(agent, query)\n",
        "    for ch in result:\n",
        "        print(ch, end=\"\")\n",
        "        sys.stdout.flush()\n",
        "        time.sleep(0.01)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "YqPTSJL6J3fU",
        "outputId": "cd9f6133-17a4-475b-b93e-96453dad8c84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Gemini API key: AIzaSyAbjgkFZNVsDvzR3JM2MCXtvMWFj7XQVUM\n",
            "\n",
            "Enter your query (or 'exit' to quit): explin try \n",
            "\n",
            "ðŸ” Tool outputs:\n",
            " - (Simulated) Web search results for: 'explin try '\n",
            " - (Simulated) File search for 'explin try ' in ['VECTOR_STORE_ID']\n",
            "\n",
            "ðŸ¤– Assistant response:\n",
            "The word \"try\" can have a few different meanings, and understanding the context will help me explain it best. Here are some common ways we use \"try\":\n",
            "\n",
            "*   **Attempt/Make an Effort:** This is the most common meaning. It means to make an effort to do something, even if you're not sure you'll succeed.\n",
            "    *   *Example:* \"I will *try* to finish the report by tomorrow.\" (This means you will make an effort to finish the report, but there's a possibility you might not.)\n",
            "*   **Test/Experiment:** To *try* something can also mean to test it out or experiment with it to see if it works or if you like it.\n",
            "    *   *Example:* \"Have you *tried* the new restaurant downtown?\" (This means, have you tested out or experienced the new restaurant to see if you like it?)\n",
            "*   **Legal Context:** In a legal setting, to \"try\" someone means to conduct a trial to determine their guilt or innocence.\n",
            "    *   *Example:* \"The suspect will be *tried* in court next month.\"\n",
            "\n",
            "**In Summary:**\n",
            "\n",
            "\"Try\" generally indicates an action taken with an uncertain outcome. It suggests effort, experimentation, or a formal process (like a trial).\n",
            "\n",
            "To give you a more tailored explanation, could you provide more context? For example:\n",
            "\n",
            "*   What were you trying to do when you encountered this word?\n",
            "*   Where did you see or hear the word \"try\"?\n",
            "\n",
            "With more context, I can give you a more precise and helpful explanation!\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2012891531.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# ------------------ Run Agent ------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEnter your query (or 'exit' to quit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Goodbye! ðŸ‘‹\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}